{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All Imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activation functions and its derivative\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self,activation=\"sigmoid\",output_activation=\"softmax\") -> None:\n",
    "        self.activation = activation\n",
    "        self.output_activation=output_activation\n",
    "        \n",
    "    def get_activation(self,a):\n",
    "        if self.activation==\"relu\":\n",
    "            return np.maximum(a,0)\n",
    "        a = np.clip(a, -200, 200)\n",
    "        if self.activation==\"tanh\":\n",
    "            return np.tanh(a)\n",
    "        if self.activation==\"sigmoid\":\n",
    "            return 1/(1+np.exp(-a))\n",
    "        \n",
    "        return 1/(1+np.exp(-a)) # by default it will take sigmoid\n",
    "        \n",
    "        \n",
    "    def get_derivative(self,a):\n",
    "        if self.activation==\"relu\":\n",
    "            a[a<=0] = 0\n",
    "            a[a>0] = 1\n",
    "            return a\n",
    "        a = np.clip(a, -200, 200)\n",
    "        if self.activation==\"tanh\":\n",
    "            return 1 - (self.get_activation(a)**2)\n",
    "        if self.activation==\"sigmoid\":\n",
    "            return self.get_activation(a) * (1 - self.get_activation(a))\n",
    "        \n",
    "        return self.get_activation(a) * (1 - self.get_activation(a)) # by default it will take sigmoid\n",
    "        \n",
    "        \n",
    "    def get_output_activation(self, a):\n",
    "        if self.output_activation==\"softmax\":\n",
    "            a = np.clip(a,-200,200)\n",
    "            return np.exp(a)/np.sum(np.exp(a))\n",
    "        \n",
    "        a = np.clip(a,-200,200)\n",
    "        return np.exp(a)/np.sum(np.exp(a))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializer class\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self, init_type=\"random\") -> None:\n",
    "        self.init_type = init_type\n",
    "        \n",
    "    def random_initialize(self, number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size):\n",
    "        if number_of_layers<=2:\n",
    "            return [np.random.randn(nodes_in_output_layer,input_layer_size)], [np.random.randn(nodes_in_output_layer)]\n",
    "\n",
    "        if number_of_layers==3:\n",
    "            Weights = [np.random.randn(nodes_per_hidden_layer, input_layer_size), np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)]\n",
    "            Biases = [np.random.randn(nodes_per_hidden_layer), np.random.randn(nodes_in_output_layer)]\n",
    "            return Weights, Biases\n",
    "\n",
    "        WS = np.random.randn(nodes_per_hidden_layer, input_layer_size)\n",
    "        W = np.random.randn(number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer)\n",
    "        B = np.random.randn(number_of_layers-2, nodes_per_hidden_layer)\n",
    "        WL = np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)\n",
    "        BL = np.random.randn(nodes_in_output_layer)\n",
    "\n",
    "        Weights = [WS] + [i for i in W] + [WL]\n",
    "        Biases = [i for i in B] + [BL]\n",
    "        return Weights, Biases\n",
    "    \n",
    "    def xavier_initialize(self,number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
    "        if number_of_layers<=2:\n",
    "            return [np.random.randn(nodes_in_output_layer,input_layer_size)], [np.random.randn(nodes_in_output_layer)]\n",
    "\n",
    "        fact_in = np.sqrt(6/(input_layer_size + nodes_per_hidden_layer))\n",
    "        fact_out = np.sqrt(6/(nodes_in_output_layer + nodes_per_hidden_layer))\n",
    "        fact_hid = np.sqrt(6/(nodes_per_hidden_layer + nodes_per_hidden_layer))\n",
    "\n",
    "        if number_of_layers==3:\n",
    "            # fact_in = np.sqrt(6/(input_layer_size + nodes_per_hidden_layer))\n",
    "            # fact_out = np.sqrt(6/(nodes_in_output_layer + nodes_per_hidden_layer))\n",
    "            Weights = [np.random.uniform(-fact_in, fact_in, (nodes_per_hidden_layer, input_layer_size)), np.random.uniform(-fact_out,fact_out,(nodes_in_output_layer, nodes_per_hidden_layer))]\n",
    "            Biases = [np.zeros(nodes_per_hidden_layer), np.zeros(nodes_in_output_layer)]\n",
    "            return Weights, Biases\n",
    "\n",
    "        WS = np.random.uniform(-fact_in, fact_in, (nodes_per_hidden_layer, input_layer_size))\n",
    "        W = np.random.uniform(-fact_hid, fact_hid, (number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer))\n",
    "        B = np.zeros([number_of_layers-2, nodes_per_hidden_layer])\n",
    "        WL = np.random.uniform(-fact_out,fact_out,(nodes_in_output_layer, nodes_per_hidden_layer))\n",
    "        BL = np.zeros(nodes_in_output_layer)\n",
    "\n",
    "        Weights = [WS] + [i for i in W] + [WL]\n",
    "        Biases = [i for i in B] + [BL]\n",
    "        return Weights, Biases\n",
    "    \n",
    "    def initialize(self,number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
    "        if self.init_type==\"xavier\":\n",
    "            return self.xavier_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size)\n",
    "        elif self.init_type==\"random\":\n",
    "            return self.random_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size)\n",
    "        return self.random_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size) # by default random initializr will be called\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preactivation class\n",
    "\n",
    "class Preactivation:\n",
    "    def __init__(self, pre_activation = \"linear\") -> None:\n",
    "        self.pre_activation = pre_activation\n",
    "        \n",
    "    def get_pre_activation(self, Weights, Biases, H):\n",
    "        if self.pre_activation==\"linear\":\n",
    "            return np.matmul(Weights, H) + Biases\n",
    "        return np.matmul(Weights, H) + Biases\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy class\n",
    "\n",
    "class Accuracy:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def get_accuracy(self, y_actual, y_predicted):\n",
    "        total = len(y_actual)\n",
    "        cnt = 0\n",
    "        for i in range(total):\n",
    "            if np.argmax(y_actual[i]) == np.argmax(y_predicted[i]):\n",
    "                cnt = cnt + 1\n",
    "\n",
    "        return (cnt/total)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss class\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, loss_type=\"cross_entropy\") -> None:\n",
    "        self.loss_type = loss_type\n",
    "    \n",
    "    def get_loss(self, y_actual, y_pred):\n",
    "        if self.loss_type==\"cross_entropy\":\n",
    "            return self.cross_entropy_loss(y_actual,y_pred)\n",
    "        if self.loss_type==\"mean_squared\":\n",
    "            return self.mean_squared_error(y_actual, y_pred)\n",
    "        if self.loss_type==\"root_mean_squared\":\n",
    "            return self.root_mean_squared(y_actual,y_pred)\n",
    "        \n",
    "        return self.cross_entropy_loss(y_actual,y_pred)\n",
    "        \n",
    "        \n",
    "    def cross_entropy_loss(self, y_actual, y_pred):\n",
    "        loss = 0\n",
    "        for i in range(len(y_actual)):\n",
    "            loss = loss + (-np.log(y_pred[i][np.argmax(y_actual[i])]))\n",
    "        return loss\n",
    "    \n",
    "    def mean_squared_error(self, y_actual, y_pred):\n",
    "        loss = 0\n",
    "        for i in range(len(y_actual)):\n",
    "            for j in range(len(y_actual[i])):\n",
    "                loss = loss + (y_actual[i][j]-y_pred[i][j])**2\n",
    "        loss = loss / (len(y_actual)*len(y_actual[0]))\n",
    "        return loss\n",
    "    \n",
    "    def root_mean_squared(self, y_actual,y_pred):\n",
    "        return np.sqrt(self.mean_squared_error(y_actual,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient_descent class\n",
    "from tqdm import tqdm\n",
    "class GradientDescent:\n",
    "    def __init__(self, optimizer) -> None:\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def update_weights_and_biases(self, learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant):\n",
    "        for i in range(len(Weights)):\n",
    "            Weights[i] = Weights[i] - learning_rate * delta_Weights[i]\n",
    "            # for j in range(len(Weights[i])):\n",
    "            #     Weights[i][j] = Weights[i][j] - learning_rate * delta_Weights[i][j] - (learning_rate * l2_regularization_constant * Weights[i][j])\n",
    "\n",
    "            for j in range(len(Biases[i])):\n",
    "                Biases[i][j] = Biases[i][j] - learning_rate * delta_Biases[i][j] #- (learning_rate * l2_regularization_constant * Biases[i][j])\n",
    "\n",
    "        return Weights, Biases\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X, Weights, Biases, number_of_layers, activation, output_activation, batch_size, pre_activation, **params):\n",
    "        A = []\n",
    "        H = [X]\n",
    "        \n",
    "        for i in range(number_of_layers-2):\n",
    "        \n",
    "            modified_bias = Biases[i].reshape(1,-1)\n",
    "            modified_bias_N = np.repeat(modified_bias, batch_size, axis=0).transpose()\n",
    "            \n",
    "            A.append(pre_activation(Weights[i],H[i],modified_bias_N))\n",
    "            H.append(activation(A[i]))\n",
    "\n",
    "        modified_bias = Biases[-1].reshape(1,-1)\n",
    "        modified_bias_N = np.repeat(modified_bias, batch_size, axis=0).transpose()\n",
    "\n",
    "        A.append(pre_activation(Weights[-1], H[-1], modified_bias_N))\n",
    "\n",
    "        y_pred_temp = []\n",
    "        A_trns = A[-1].transpose()\n",
    "        for i in range(batch_size):\n",
    "            y_pred_temp.append(output_activation(A_trns[i]))\n",
    "\n",
    "        y_pred = np.array(y_pred_temp).transpose()\n",
    "\n",
    "        return H, A, y_pred\n",
    "    \n",
    "    def backward_propagation(self, H, A, W, y_actual, y_pred, number_of_layers, derivative, **params):\n",
    "        \n",
    "        delta_W = [0 for i in range(number_of_layers-1)]\n",
    "        delta_B = [0 for i in range(number_of_layers-1)]\n",
    "\n",
    "        delta_A = -(y_actual-y_pred)\n",
    "        delta_H = None\n",
    "\n",
    "        for k in reversed(range(number_of_layers-1)):\n",
    "\n",
    "            # gradient with respect to parameters\n",
    "            delta_W[k] = np.matmul(delta_A, H[k].transpose())\n",
    "\n",
    "            delta_B[k] = np.sum(delta_A,axis=1)\n",
    "\n",
    "            if k==0:\n",
    "                break\n",
    "            \n",
    "            # gradient with respect to layer below\n",
    "            delta_H = np.matmul(W[k].transpose() , delta_A)\n",
    "\n",
    "            #gradient with respect to layer below (i.e. pre-activation)\n",
    "            delta_A = np.multiply(delta_H,[derivative(i) for i in A[k-1]])            \n",
    "\n",
    "        return delta_W, delta_B\n",
    "\n",
    "    def run_gradient_descent(self, X, Y, learning_rate, num_of_layers, batch_size, Weights, Biases, activation, l2_reg_constant, beta, epsilon, output_activation, pre_activation, derivative):\n",
    "        if self.optimizer==\"sgd\" or self.optimizer==\"mini_batch\":\n",
    "            return self.gradient_descent_mini_batch(X=X,Y=Y,learning_rate=learning_rate,number_of_layers=num_of_layers,batch_size=batch_size,Weights=Weights,Biases=Biases,activation_function=activation,l2_regularization_constant=l2_reg_constant,output_activation=output_activation,pre_activation=pre_activation,derivative=derivative)\n",
    "        elif self.optimizer==\"mbgd\":\n",
    "            return self.gradient_descent_mbgd(X=X,Y=Y,learning_rate=learning_rate,number_of_layers=num_of_layers,batch_size=batch_size,Weights=Weights,Biases=Biases,activation_function=activation,l2_regularization_constant=l2_reg_constant,beta=beta,output_activation=output_activation,pre_activation=pre_activation,derivative=derivative)\n",
    "        elif self.optimizer==\"rmsprop\":\n",
    "            return self.gradient_descent_rmsprop(X=X,Y=Y,learning_rate=learning_rate,number_of_layers=num_of_layers,batch_size=batch_size,Weights=Weights,Biases=Biases,activation_function=activation,l2_regularization_constant=l2_reg_constant,beta=beta,epsilon=epsilon,output_activation=output_activation,pre_activation=pre_activation,derivative=derivative)\n",
    "        elif self.optimizer==\"nagd\":\n",
    "            return self.gradient_descent_nagd(X=X,Y=Y,learning_rate=learning_rate,number_of_layers=num_of_layers,batch_size=batch_size,Weights=Weights,Biases=Biases,activation_function=activation,l2_regularization_constant=l2_reg_constant,beta=beta,output_activation=output_activation,pre_activation=pre_activation,derivative=derivative)\n",
    "        elif self.optimizer==\"adam\":\n",
    "            pass\n",
    "        elif self.optimizer==\"nadam\":\n",
    "            pass\n",
    "        \n",
    "\n",
    "    \n",
    "    def gradient_descent_mini_batch(self,X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, output_activation, pre_activation,derivative):\n",
    "        itr = 0\n",
    "\n",
    "        for itr in tqdm(range(X.shape[0]//batch_size)):\n",
    "            H, A, y_pred = self.forward_propagation(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function, output_activation, batch_size,pre_activation)\n",
    "\n",
    "            delta_Weights, delta_Biases = self.backward_propagation(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, derivative)\n",
    "\n",
    "            Weights, Biases = self.update_weights_and_biases(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant)\n",
    "\n",
    "        return Weights, Biases\n",
    "    \n",
    "    def gradient_descent_mbgd(self, X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, output_activation, pre_activation,derivative):\n",
    "        \n",
    "        def __accumulate_history(prev, current, prev_factor=1):\n",
    "            temp = []\n",
    "            for i in range(len(prev)):\n",
    "                temp.append((prev[i]*prev_factor) + (current[i]))\n",
    "\n",
    "            return temp\n",
    "        \n",
    "        itr = 0\n",
    "        u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
    "        u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
    "    \n",
    "        for itr in tqdm(range(X.shape[0])):\n",
    "            H, A, y_pred = self.forward_propagation(X[itr*batch_size:(itr+1)*batch_size], Weights, Biases, number_of_layers, activation_function,output_activation,batch_size,pre_activation)\n",
    "            delta_Weights, delta_Biases = self.backward_propagation(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size], y_pred, number_of_layers, derivative)\n",
    "            # u_t = beta * u_t + delta_Weights\n",
    "            u_t_weights = __accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
    "            u_t_biases = __accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
    "\n",
    "            \n",
    "            Weights, Biases = self.update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
    "\n",
    "        return Weights, Biases\n",
    "    \n",
    "    def gradient_descent_rmsprop(self, X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon, output_activation, pre_activation,derivative):\n",
    "        \n",
    "        def __accumulate_history(prev, current, prev_factor,current_factor):\n",
    "            temp = []\n",
    "            for i in range(len(prev)):\n",
    "                temp.append((prev[i]*prev_factor) + (current[i]*current_factor))\n",
    "\n",
    "            return temp\n",
    "        \n",
    "        def __square_each_term(a):\n",
    "            temp = []\n",
    "            for i in range(len(a)):\n",
    "                temp.append(np.array(a[i])**2)\n",
    "            return temp\n",
    "\n",
    "        def __modify_deltas_RMSProp(v_t, w_t, epsilon):\n",
    "            temp = []\n",
    "            for i in range(len(v_t)):\n",
    "                temp.append(w_t[i] / (np.sqrt(v_t[i]) + epsilon))\n",
    "            return temp\n",
    "        \n",
    "        itr = 0\n",
    "        v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
    "        v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
    "\n",
    "        for itr in tqdm(range(X.shape[0])):\n",
    "\n",
    "            H, A, y_pred = self.forward_propagation(X[itr*batch_size:(itr+1)*batch_size], Weights, Biases, number_of_layers, activation_function,output_activation,batch_size,pre_activation)\n",
    "            delta_Weights, delta_Biases = self.backward_propagation(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size], y_pred, number_of_layers, derivative)\n",
    "\n",
    "            v_t_weights = __accumulate_history(v_t_weights,__square_each_term(delta_Weights),prev_factor=beta, current_factor=1-beta)\n",
    "            v_t_biases = __accumulate_history(v_t_biases,__square_each_term(delta_Biases), prev_factor=beta, current_factor=1-beta)\n",
    "\n",
    "            Weights, Biases = self.update_weights_and_biases(learning_rate, Weights, Biases, __modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), __modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
    "\n",
    "\n",
    "        return Weights, Biases\n",
    "    \n",
    "    def gradient_descent_nagd(self,X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, output_activation, pre_activation,derivative):\n",
    "        def __modify_W_B_NAGD(u_t, w_t, beta):\n",
    "            temp = []\n",
    "            for i in range(len(u_t)):\n",
    "                temp.append(w_t[i]- (beta*u_t[i]))\n",
    "            return temp\n",
    "        \n",
    "        def __accumulate_history(prev, current, prev_factor=1, current_factor=1):\n",
    "            temp = []\n",
    "            for i in range(len(prev)):\n",
    "                temp.append((prev[i]*prev_factor) + (current[i]*current_factor))\n",
    "\n",
    "            return temp\n",
    "        \n",
    "        itr = 0\n",
    "        u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
    "        u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
    "\n",
    "        for itr in tqdm(range(X.shape[0]//batch_size)):\n",
    "            H, A, y_pred = self.forward_propagation(X[itr*batch_size:(itr+1)*batch_size].transpose(), __modify_W_B_NAGD(u_t_weights, Weights, beta), __modify_W_B_NAGD(u_t_biases, Biases, beta), number_of_layers, activation_function, output_activation, batch_size,pre_activation)\n",
    "            delta_Weights, delta_Biases = self.backward_propagation(H, A, __modify_W_B_NAGD(u_t_weights, Weights, beta), Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, derivative)\n",
    "\n",
    "            u_t_weights = __accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
    "            u_t_biases = __accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
    "\n",
    "            Weights, Biases = self.update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
    "\n",
    "\n",
    "        return Weights, Biases\n",
    "        \n",
    "    \n",
    "    def gradient_descent_adam():\n",
    "        pass\n",
    "    \n",
    "    def gradient_descent_nadam():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FFNN model\n",
    "\n",
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, X, Y, epochs, num_of_hidden_layer, size_of_layer, learning_rate, optimizer, batch_size, l2_reg_constant, weight_init_type, activation_function, beta, epsilon, loss_type) -> None:\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_validation = X[54000:]\n",
    "        self.Y_validation = Y[54000:]\n",
    "        self.X_train = X[:54000]\n",
    "        self.Y_train = Y[:54000]\n",
    "        self.epochs = epochs\n",
    "        self.num_of_hidden_layer = num_of_hidden_layer\n",
    "        self.num_of_layers = num_of_hidden_layer+2\n",
    "        self.size_of_layer = size_of_layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_init_type = weight_init_type\n",
    "        self.l2_reg_constant = l2_reg_constant\n",
    "        self.activation_function = activation_function\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        # self.X = self.X.reshape(self.X.shape[0],-1)/255\n",
    "        self.X_train = self.X_train.reshape(self.X_train.shape[0],-1)/255\n",
    "        self.X_validation = self.X_validation.reshape(self.X_validation.shape[0],-1)/255\n",
    "        \n",
    "        ty = [np.zeros(10) for i in range(self.X_train.shape[0])]\n",
    "        for i in range(self.X_train.shape[0]):\n",
    "            ty[i][self.Y_train[i]] = 1\n",
    "            \n",
    "        vy = [np.zeros(10) for i in range(self.X_validation.shape[0])]\n",
    "        for i in range(self.X_validation.shape[0]):\n",
    "            vy[i][self.Y_validation[i]] = 1\n",
    "            \n",
    "        self.Y_train = np.array(ty)\n",
    "        self.Y_validation = np.array(vy)\n",
    "            \n",
    "        \n",
    "        initializer = Initializer(self.weight_init_type)\n",
    "        self.Weights, self.Biases = initializer.initialize(self.num_of_layers,self.size_of_layer,self.Y_train.shape[1], self.X_train.shape[1])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def train_model(self):\n",
    "        print(\"number of training datapoints:\",self.X_train.shape[0])\n",
    "        print(\"number of validation datapoints:\",self.X_validation.shape[0])\n",
    "        print(\"number of epochs:\", self.epochs)\n",
    "        print(\"number of hidden layers:\", self.num_of_hidden_layer)\n",
    "        print(\"size of hidden layers:\", self.size_of_layer)\n",
    "        print(\"learning rate:\", self.learning_rate)\n",
    "        print(\"optimizer:\", self.optimizer)\n",
    "        print(\"batch_size:\", self.batch_size)\n",
    "        print(\"l2 regularization constant:\", self.l2_reg_constant)\n",
    "        print(\"weights and biases initialization type:\", self.weight_init_type)\n",
    "        print(\"activation function:\", self.activation_function)\n",
    "        print(\"beta:\", self.beta)\n",
    "        print(\"epsilon:\", self.epsilon)\n",
    "        \n",
    "        activation_class = Activation(self.activation_function,\"softmax\")\n",
    "        derivative = activation_class.get_derivative\n",
    "        activation =activation_class.get_activation\n",
    "        output_activation = activation_class.get_output_activation\n",
    "        \n",
    "        pre_activation = Preactivation(\"linear\").get_pre_activation\n",
    "        \n",
    "        # initializer = Initializer(self.weight_init_type)\n",
    "        # Weights, Biases = initializer.initialize(self.num_of_hidden_layer+2,self.size_of_layer,self.Y.shape[1], self.X.shape[1])\n",
    "\n",
    "\n",
    "        loss = Loss(self.loss_type)\n",
    "\n",
    "\n",
    "        accuracy = Accuracy()\n",
    "        gradient_descent = GradientDescent(self.optimizer)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"Epoch number\", epoch+1, \" started\")\n",
    "            self.Weights, self.Biases = gradient_descent.run_gradient_descent(self.X, self.Y, self.learning_rate, self.num_of_hidden_layer+2, self.batch_size, self.Weights, self.Biases, activation, self.l2_reg_constant, self.beta, self.epsilon, output_activation, pre_activation, derivative)\n",
    "            print(\"Epoch \",epoch+1, \" finished.\")\n",
    "            # _,_, Y_validation_predict = gradient_descent.forward_propagation(self.X_validation, Weights, Biases, self.num_of_hidden_layer+2, activation, output_activation, self.X_validation.shape[0],pre_activation)\n",
    "            # _,_, Y_train_predict = gradient_descent.forward_propagation(self.X_train, Weights, Biases, self.num_of_hidden_layer+2, activation, output_activation, self.X_validation.shape[0],pre_activation)\n",
    "\n",
    "            # print(\"validation accuracy after epoch\",epoch+1,accuracy.get_accuracy(self.Y_validation,Y_predict))\n",
    "\n",
    "        return self.Weights, self.Biases\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        if X_test.shape[1]==28:\n",
    "            X_test = X_test.reshape(X_test.shape[0],-1)/255\n",
    "        gradient_descent = GradientDescent(self.optimizer)\n",
    "        act = Activation(self.activation_function, \"softmax\")\n",
    "        pre = Preactivation(\"linear\")\n",
    "        _,_, Y_test_predict = gradient_descent.forward_propagation(X_test, self.Weights, self.Biases, self.num_of_layers, act.get_activation, act.get_output_activation, X_test.shape[0],pre.get_pre_activation)\n",
    "        return Y_test_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
