{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR9Ghl_O55AV"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbqIOG0i55AX"
      },
      "outputs": [],
      "source": [
        "# All functions required\n",
        "\n",
        "# Activation functions\n",
        "# 1 Relu\n",
        "def relu(a):\n",
        "    return np.max(a,0)\n",
        "\n",
        "def relu_vector_old(a):\n",
        "    temp = []\n",
        "    for i in range(len(a)):\n",
        "        temp.append(relu(a[i]))\n",
        "    return temp\n",
        "\n",
        "def relu_vector(a):\n",
        "    return np.maximum(a,0)\n",
        "\n",
        "\n",
        "# 2 Sigmoid\n",
        "def sigmoid(a):\n",
        "    ans = 1/(1+np.exp(-a))\n",
        "    return ans\n",
        "\n",
        "def sigmoid_vector_old(a):\n",
        "    temp=[]\n",
        "    for i in range(len(a)):\n",
        "        temp.append(sigmoid(a[i]))\n",
        "    return temp\n",
        "\n",
        "def sigmoid_vector(a):\n",
        "    a = np.clip(a, -200,200)\n",
        "    ans = 1/(1+np.exp(-a))\n",
        "    return ans\n",
        "\n",
        "# 3 Tanh\n",
        "def tanh(a):\n",
        "    a = np.clip(a, -200, 200)\n",
        "    ans = (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
        "    return ans\n",
        "\n",
        "def tanh_vector_old(a):\n",
        "    temp = []\n",
        "    for i in range(len(a)):\n",
        "        temp.append(tanh(a[i]))\n",
        "    return temp\n",
        "\n",
        "def tanh_vector(a):\n",
        "    return np.tanh(a)\n",
        "    # a = np.clip(a, -200, 200)\n",
        "    # ans = (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
        "    # return ans\n",
        "\n",
        "def identity(a):\n",
        "    return a\n",
        "\n",
        "\n",
        "# Output Activation Function\n",
        "\n",
        "# Softmax\n",
        "def softmax(a):\n",
        "    a = np.clip(a, -200, 200)\n",
        "    return np.exp(a)/np.sum(np.exp(a))\n",
        "\n",
        "#-------------------------------------------------------\n",
        "# utility functions\n",
        "\n",
        "def hadamard_product(A,B):\n",
        "    # result = []\n",
        "    # for i in range(len(A)):\n",
        "    #     result.append(A[i]*B[i])\n",
        "    return np.multiply(np.array(A),np.array(B))\n",
        "    # return result\n",
        "\n",
        "def random_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
        "    if number_of_layers<=2:\n",
        "        return [np.random.randn(nodes_in_output_layer,input_layer_size)], [np.random.randn(nodes_in_output_layer)]\n",
        "\n",
        "    if number_of_layers==3:\n",
        "        Weights = [np.random.randn(nodes_per_hidden_layer, input_layer_size), np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)]\n",
        "        Biases = [np.random.randn(nodes_per_hidden_layer), np.random.randn(nodes_in_output_layer)]\n",
        "        return Weights, Biases\n",
        "\n",
        "    WS = np.random.randn(nodes_per_hidden_layer, input_layer_size)*0.1\n",
        "    W = np.random.randn(number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer)*0.1\n",
        "    B = np.random.randn(number_of_layers-2, nodes_per_hidden_layer)*0.1\n",
        "    WL = np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)*0.1\n",
        "    BL = np.random.randn(nodes_in_output_layer)*0.1\n",
        "\n",
        "    Weights = [WS] + [i for i in W] + [WL]\n",
        "    Biases = [i for i in B] + [BL]\n",
        "    return Weights, Biases\n",
        "\n",
        "def xavier_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
        "    if number_of_layers<=2:\n",
        "        return [np.random.randn(nodes_in_output_layer,input_layer_size)], [np.random.randn(nodes_in_output_layer)]\n",
        "\n",
        "    fact_in = np.sqrt(6/(input_layer_size + nodes_per_hidden_layer))\n",
        "    fact_out = np.sqrt(6/(nodes_in_output_layer + nodes_per_hidden_layer))\n",
        "    fact_hid = np.sqrt(6/(nodes_per_hidden_layer + nodes_per_hidden_layer))\n",
        "\n",
        "    if number_of_layers==3:\n",
        "        # fact_in = np.sqrt(6/(input_layer_size + nodes_per_hidden_layer))\n",
        "        # fact_out = np.sqrt(6/(nodes_in_output_layer + nodes_per_hidden_layer))\n",
        "        Weights = [np.random.uniform(-fact_in, fact_in, (nodes_per_hidden_layer, input_layer_size)), np.random.uniform(-fact_out,fact_out,(nodes_in_output_layer, nodes_per_hidden_layer))]\n",
        "        Biases = [np.zeros(nodes_per_hidden_layer), np.zeros(nodes_in_output_layer)]\n",
        "        return Weights, Biases\n",
        "\n",
        "    WS = np.random.uniform(-fact_in, fact_in, (nodes_per_hidden_layer, input_layer_size))\n",
        "    W = np.random.uniform(-fact_hid, fact_hid, (number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer))\n",
        "    B = np.zeros([number_of_layers-2, nodes_per_hidden_layer])\n",
        "    WL = np.random.uniform(-fact_out,fact_out,(nodes_in_output_layer, nodes_per_hidden_layer))\n",
        "    BL = np.zeros(nodes_in_output_layer)\n",
        "\n",
        "    Weights = [WS] + [i for i in W] + [WL]\n",
        "    Biases = [i for i in B] + [BL]\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_accuracy(Y_actual, Y_predicted):\n",
        "    total = len(Y_actual)\n",
        "    print(\"total data points in validation \",total)\n",
        "    cnt = 0\n",
        "    for i in range(total):\n",
        "        if np.argmax(Y_actual[i]) == np.argmax(Y_predicted[i]):\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    return (cnt/total)*100\n",
        "\n",
        "\n",
        "def get_average_delta_WandB(delta_W_acc, delta_B_acc):\n",
        "    for i in range(1,len(delta_W_acc)):\n",
        "        for j in range(len(delta_W_acc[0])):\n",
        "            delta_W_acc[0][j] = np.add(delta_W_acc[0][j] , delta_W_acc[i][j])\n",
        "            # if i==len(delta_W_acc)-1:\n",
        "            #     delta_W_acc[0][j] = delta_W_acc[0][j] / len(delta_W_acc)\n",
        "\n",
        "        for j in range(len(delta_B_acc[0])):\n",
        "            delta_B_acc[0][j] = np.add(delta_B_acc[0][j] , delta_B_acc[i][j])\n",
        "            # if i==len(delta_B_acc)-1:\n",
        "            #     delta_B_acc[0][j] = delta_B_acc[0][j] / len(delta_B_acc)\n",
        "\n",
        "    return delta_W_acc[0], delta_B_acc[0]\n",
        "\n",
        "\n",
        "#---------------------------------------------------\n",
        "\n",
        "# Derivative functions\n",
        "\n",
        "def derivative_sigmoid(a):\n",
        "    return sigmoid(a) * (1-sigmoid(a))\n",
        "\n",
        "def derivative_tanh(a):\n",
        "    return 1 - (tanh(a)**2)\n",
        "\n",
        "def derivative_relu(a):\n",
        "    # if a<=0:\n",
        "    #     return 0\n",
        "    a[a<=0] = 0\n",
        "    a[a>0] = 1\n",
        "    return a\n",
        "\n",
        "def derivative_identity(a):\n",
        "    return np.ones_like(a)\n",
        "\n",
        "\n",
        "#---------------------------------------------------\n",
        "# Pre Avtivation function\n",
        "\n",
        "\n",
        "def pre_activation(W, h, b):\n",
        "    # print(W.shape)\n",
        "    # print(h.shape)\n",
        "    # print(b.shape)\n",
        "    return np.add(np.matmul(W,h) , b)\n",
        "\n",
        "\n",
        "#------------------------------------------------------\n",
        "\n",
        "# forward propagation\n",
        "\n",
        "\n",
        "# def forward_propagation(X, Weights, Biases, number_of_layers, activation_function):\n",
        "\n",
        "#     if activation_function==\"relu\":\n",
        "#         activation = relu_vector\n",
        "#     elif activation_function==\"tanh\":\n",
        "#         activation = tanh_vector\n",
        "#     else:\n",
        "#         activation = sigmoid_vector\n",
        "\n",
        "#     A = []\n",
        "#     H = [X]\n",
        "#     for i in range(number_of_layers-2):\n",
        "#         A.append(pre_activation(Weights[i],H[i],Biases[i]))\n",
        "#         H.append(activation(A[i]))\n",
        "\n",
        "#     A.append(pre_activation(Weights[-1], H[-1], Biases[-1]))\n",
        "\n",
        "#     y_pred = softmax(A[-1])\n",
        "\n",
        "#     return H, A, y_pred\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "# Backward Propagation\n",
        "\n",
        "# def backward_propagation(H, A, W, y_actual, y_pred, number_of_layers, activation_function):\n",
        "\n",
        "#     if activation_function==\"relu\":\n",
        "#         derivative = derivative_relu\n",
        "#     elif activation_function==\"tanh\":\n",
        "#         derivative = derivative_tanh\n",
        "#     else:\n",
        "#         derivative = derivative_sigmoid\n",
        "#     # delta_A = [0 for i in range(number_of_layers-1)]\n",
        "#     delta_W = [0 for i in range(number_of_layers-1)]\n",
        "#     delta_B = [0 for i in range(number_of_layers-1)]\n",
        "#     # delta_H = [0 for i in range(number_of_layers-2)]\n",
        "\n",
        "#     # gradient with respect to output\n",
        "#     # delta_A[-1] = -(y_actual-y_pred)\n",
        "#     delta_A = -(y_actual-y_pred)\n",
        "#     delta_H = None\n",
        "\n",
        "\n",
        "#     for k in reversed(range(number_of_layers-1)):\n",
        "\n",
        "#         # gradient with respect to parameters\n",
        "#         # delta_W[k] = np.outer(delta_A[k],H[k-1])\n",
        "#         delta_W[k] = np.outer(delta_A, H[k])\n",
        "#         # delta_B[k] = delta_A[k]\n",
        "#         delta_B[k] = delta_A\n",
        "\n",
        "#         if k==0:\n",
        "#             break\n",
        "#         # gradient with respect to layer below\n",
        "#         # delta_H[k-1] = np.matmul(W[k].transpose() , delta_A[k])\n",
        "#         delta_H = np.matmul(W[k].transpose() , delta_A)\n",
        "\n",
        "#         #gradient with respect to layer below (i.e. pre-activation)\n",
        "#         # delta_A[k-1] = hadamard_product(delta_H[k-1],[derivative(i) for i in A[k-1]])\n",
        "#         delta_A = hadamard_product(delta_H,[derivative(i) for i in A[k-1]])\n",
        "\n",
        "\n",
        "#     return delta_W, delta_B\n",
        "\n",
        "\n",
        "#-------------------------------------------\n",
        "\n",
        "# new optimization\n",
        "\n",
        "\n",
        "def forward_propagation_n(X, Weights, Biases, number_of_layers, activation_function, batch_size):\n",
        "\n",
        "    if activation_function==\"relu\":\n",
        "        activation = relu_vector\n",
        "    elif activation_function==\"tanh\":\n",
        "        activation = tanh_vector\n",
        "    elif activation_function==\"identity\":\n",
        "        activation = identity\n",
        "    else:\n",
        "        activation = sigmoid_vector\n",
        "\n",
        "    A = []\n",
        "    H = [X]\n",
        "    for i in range(number_of_layers-2):\n",
        "\n",
        "        modified_bias = Biases[i].reshape(1,-1)\n",
        "        modified_bias_N = np.repeat(modified_bias, batch_size, axis=0).transpose()\n",
        "\n",
        "        A.append(pre_activation(Weights[i],H[i],modified_bias_N))\n",
        "        H.append(activation(A[i]))\n",
        "\n",
        "    modified_bias = Biases[-1].reshape(1,-1)\n",
        "    modified_bias_N = np.repeat(modified_bias, batch_size, axis=0).transpose()\n",
        "\n",
        "    A.append(pre_activation(Weights[-1], H[-1], modified_bias_N))\n",
        "\n",
        "    y_pred_temp = []\n",
        "    A_trns = A[-1].transpose()\n",
        "    for i in range(batch_size):\n",
        "        y_pred_temp.append(softmax(A_trns[i]))\n",
        "\n",
        "    y_pred = np.array(y_pred_temp).transpose()\n",
        "\n",
        "    return H, A, y_pred\n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "\n",
        "def backward_propagation_n(H, A, W, y_actual, y_pred, number_of_layers, activation_function,loss_type):\n",
        "\n",
        "    if activation_function==\"relu\":\n",
        "        derivative = derivative_relu\n",
        "    elif activation_function==\"tanh\":\n",
        "        derivative = derivative_tanh\n",
        "    elif activation_function==\"identity\":\n",
        "        derivative = derivative_identity\n",
        "    else:\n",
        "        derivative = derivative_sigmoid\n",
        "    # delta_A = [0 for i in range(number_of_layers-1)]\n",
        "    delta_W = [0 for i in range(number_of_layers-1)]\n",
        "    delta_B = [0 for i in range(number_of_layers-1)]\n",
        "    # delta_H = [0 for i in range(number_of_layers-2)]\n",
        "\n",
        "    # gradient with respect to output\n",
        "    # delta_A[-1] = -(y_actual-y_pred)\n",
        "\n",
        "    if loss_type==\"mse\" or loss_type==\"mean_squared_error\":\n",
        "        delta_A  = (y_pred - y_actual) * y_pred * (1 - y_pred)\n",
        "    else:\n",
        "        delta_A = -(y_actual-y_pred)\n",
        "\n",
        "    delta_H = None\n",
        "\n",
        "\n",
        "    for k in reversed(range(number_of_layers-1)):\n",
        "\n",
        "        # gradient with respect to parameters\n",
        "        # delta_W[k] = np.outer(delta_A[k],H[k])\n",
        "        delta_W[k] = np.matmul(delta_A, H[k].transpose())\n",
        "        # delta_B[k] = delta_A[k]\n",
        "        delta_B[k] = np.sum(delta_A,axis=1)\n",
        "\n",
        "        if k==0:\n",
        "            break\n",
        "        # gradient with respect to layer below\n",
        "        # delta_H[k-1] = np.matmul(W[k].transpose() , delta_A[k])\n",
        "        delta_H = np.matmul(W[k].transpose() , delta_A)\n",
        "\n",
        "        #gradient with respect to layer below (i.e. pre-activation)\n",
        "        # delta_A[k-1] = hadamard_product(delta_H[k-1],[derivative(i) for i in A[k-1]])\n",
        "        delta_A = hadamard_product(delta_H,[derivative(i) for i in A[k-1]])\n",
        "\n",
        "        # delta_B[k] = np.sum(delta_B[k],axis=0)\n",
        "\n",
        "\n",
        "    return delta_W, delta_B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "\n",
        "\n",
        "# gradient descent algorithms\n",
        "\n",
        "\n",
        "def update_weights_and_biases_n(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant):\n",
        "\n",
        "    for i in range(len(Weights)):\n",
        "\n",
        "        for j in range(len(Weights[i])):\n",
        "            Weights[i][j] = Weights[i][j] - learning_rate * delta_Weights[i][j] - (learning_rate * l2_regularization_constant * Weights[i][j])\n",
        "\n",
        "        for j in range(len(Biases[i])):\n",
        "            Biases[i][j] = Biases[i][j] - learning_rate * delta_Biases[i][j] #- (learning_rate * l2_regularization_constant * Biases[i][j])\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "# def update_weights_and_biases(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant):\n",
        "#     for i in range(len(Weights)):\n",
        "#         # Weights[i] = Weights[i] - learning_rate * delta_Weights[i]\n",
        "#         # Biases[i] = Biases[i] - learning_rate * delta_Biases[i]\n",
        "#         for j in range(len(Weights[i])):\n",
        "#             Weights[i][j] = Weights[i][j] - learning_rate * delta_Weights[i][j] - (learning_rate * l2_regularization_constant * Weights[i][j])\n",
        "\n",
        "#         for j in range(len(Biases[i])):\n",
        "#             Biases[i][j] = Biases[i][j] - learning_rate * delta_Biases[i][j] #- (learning_rate * l2_regularization_constant * Biases[i][j])\n",
        "\n",
        "#     return Weights, Biases\n",
        "\n",
        "\n",
        "# def gradient_descent_stochastic(X, Y, learning_rate, number_of_layers, batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, epsilon=0):\n",
        "\n",
        "#     # Weights, Biases = random_initialize(number_of_layers,nodes_per_hidden_layer,nodes_in_output_layer)\n",
        "#     # itr = 0\n",
        "\n",
        "#     for itr in tqdm(range(X.shape[0])):\n",
        "#         # H, A, y_pred = forward_propagation(X[itr*batch_size:(itr+1)*batch_size], Weights, Biases, number_of_layers)\n",
        "#         H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "#         # return None, None\n",
        "#         # delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size], y_pred, number_of_layers)\n",
        "#         delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "#         Weights , Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant)\n",
        "#         # itr = itr + 1\n",
        "#     return Weights, Biases\n",
        "\n",
        "# def gradient_descent_mini_batch(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, epsilon=0):\n",
        "#     itr = 0\n",
        "#     delta_W_acc = []\n",
        "#     delta_B_acc = []\n",
        "\n",
        "#     for itr in tqdm(range(X.shape[0])):\n",
        "#         H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "\n",
        "#         delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "\n",
        "#         delta_W_acc.append(delta_Weights)\n",
        "#         delta_B_acc.append(delta_Biases)\n",
        "\n",
        "#         # itr = itr + 1\n",
        "#         if (itr+1)%batch_size==0:\n",
        "#             delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "#             Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
        "\n",
        "#             delta_W_acc = []\n",
        "#             delta_B_acc = []\n",
        "#             delta_W_avg = 0\n",
        "#             delta_B_avg = 0\n",
        "\n",
        "\n",
        "#     if delta_B_acc and delta_W_acc:\n",
        "#         delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "#         Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
        "\n",
        "#     return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_mini_batch_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, beta1=0, epsilon=0, loss_type=\"cross_entropy\"):\n",
        "    itr = 0\n",
        "    # delta_W_acc = []\n",
        "    # delta_B_acc = []\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function, batch_size)\n",
        "\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function, loss_type)\n",
        "\n",
        "        # delta_W_acc.append(delta_Weights)\n",
        "        # delta_B_acc.append(delta_Biases)\n",
        "\n",
        "        # itr = itr + 1\n",
        "        # if (itr+1)%batch_size==0:\n",
        "        # delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant)\n",
        "\n",
        "        # delta_W_acc = []\n",
        "        # delta_B_acc = []\n",
        "        # delta_W_avg = 0\n",
        "        # delta_B_avg = 0\n",
        "\n",
        "    # if X.shape[0]-itr\n",
        "\n",
        "    # if delta_B_acc and delta_W_acc:\n",
        "    #     delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "        # Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "def accumulate_history(prev, current, prev_factor=1, current_factor=1):\n",
        "    temp = []\n",
        "    for i in range(len(prev)):\n",
        "        temp.append((prev[i]*prev_factor) + (current[i]*current_factor))\n",
        "\n",
        "    return temp\n",
        "\n",
        "# def gradient_descent_momentum_based(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon=0):\n",
        "\n",
        "#     itr = 0\n",
        "#     u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "#     u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "#     # u_t_list = [u_t]\n",
        "#     for itr in tqdm(range(X.shape[0])):\n",
        "#         H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "#         delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "#         # u_t = beta * u_t + delta_Weights\n",
        "#         u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "#         u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "#         # u_t_list.append()\n",
        "#         # itr = itr + 1\n",
        "\n",
        "#         if (itr+1)%batch_size==0:\n",
        "#             Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "#     if itr%batch_size!=0:\n",
        "#         Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "#     return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_momentum_based_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon=0, loss_type=\"cross_entropy\"):\n",
        "\n",
        "    itr = 0\n",
        "    u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    # u_t_list = [u_t]\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function, loss_type)\n",
        "        # u_t = beta * u_t + delta_Weights\n",
        "        u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "        u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "        # u_t_list.append()\n",
        "        # itr = itr + 1\n",
        "\n",
        "        # if (itr+1)%batch_size==0:\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "    # if itr%batch_size!=0:\n",
        "    #     Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def square_each_term(a):\n",
        "    temp = []\n",
        "    for i in range(len(a)):\n",
        "        temp.append(np.array(a[i])**2)\n",
        "    return temp\n",
        "\n",
        "def modify_deltas_RMSProp(v_t, w_t, epsilon):\n",
        "    temp = []\n",
        "    for i in range(len(v_t)):\n",
        "        temp.append(w_t[i] / (np.sqrt(v_t[i]) + epsilon))\n",
        "    return temp\n",
        "\n",
        "# def gradient_descent_RMSProp(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon):\n",
        "\n",
        "#     itr = 0\n",
        "#     v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "#     v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "#     for itr in tqdm(range(X.shape[0])):\n",
        "\n",
        "#         H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "#         delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "\n",
        "#         v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta, current_factor=1-beta)\n",
        "#         v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta, current_factor=1-beta)\n",
        "\n",
        "#         if (itr+1)%batch_size==0:\n",
        "#             Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "#     if itr%batch_size!=0:\n",
        "#         Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "#     return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_RMSProp_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon, loss_type=\"cross_entropy\"):\n",
        "\n",
        "    itr = 0\n",
        "    v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function, loss_type)\n",
        "\n",
        "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta, current_factor=1-beta)\n",
        "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta, current_factor=1-beta)\n",
        "\n",
        "        # if (itr+1)%batch_size==0:\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    # if itr%batch_size!=0:\n",
        "    #     Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def modify_W_B_NAGD(u_t, w_t, beta):\n",
        "    temp = []\n",
        "    for i in range(len(u_t)):\n",
        "        temp.append(w_t[i]- (beta*u_t[i]))\n",
        "    return temp\n",
        "\n",
        "\n",
        "# def gradient_descent_nesterov_accelarated(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon=0):\n",
        "#     itr = 0\n",
        "#     u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "#     u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "#     # u_t_list = [u_t]\n",
        "#     for itr in tqdm(range(X.shape[0])):\n",
        "#         H, A, y_pred = forward_propagation(X[itr], modify_W_B_NAGD(u_t_weights, Weights, beta), modify_W_B_NAGD(u_t_biases, Biases, beta), number_of_layers, activation_function)\n",
        "#         delta_Weights, delta_Biases = backward_propagation(H, A, modify_W_B_NAGD(u_t_weights, Weights, beta), Y[itr], y_pred, number_of_layers, activation_function)\n",
        "#         # u_t = beta * u_t + delta_Weights\n",
        "#         u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "#         u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "#         # u_t_list.append()\n",
        "#         # itr = itr + 1\n",
        "\n",
        "#         if (itr+1)%batch_size==0:\n",
        "#             Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "#     if itr%batch_size!=0:\n",
        "#         Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "\n",
        "#     return Weights, Biases\n",
        "\n",
        "# def gradient_descent_nesterov_accelarated_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon=0):\n",
        "#     itr = 0\n",
        "#     u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "#     u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "#     for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "#         H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), modify_W_B_NAGD(u_t_weights, Weights, beta), modify_W_B_NAGD(u_t_biases, Biases, beta), number_of_layers, activation_function, batch_size)\n",
        "#         delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "\n",
        "#         u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "#         u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "\n",
        "#         Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "\n",
        "#     return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_nesterov_accelarated_n1(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon=0, loss_type=\"cross_entropy\"):\n",
        "    itr = 0\n",
        "    g_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    g_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function, batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function, loss_type)\n",
        "\n",
        "\n",
        "        g_t_weights = delta_Weights\n",
        "        g_t_biases = delta_Biases\n",
        "\n",
        "        # m_t_weights = beta * m_t_weights + g_t_weights\n",
        "        # m_t_biases = beta * m_t_biases + g_t_biases\n",
        "\n",
        "        m_t_weights = accumulate_history(m_t_weights,g_t_weights,prev_factor=beta)\n",
        "        m_t_biases = accumulate_history(m_t_biases,g_t_biases, prev_factor=beta)\n",
        "\n",
        "\n",
        "        u_t_weights = accumulate_history(m_t_weights,g_t_weights,prev_factor=beta)\n",
        "        u_t_biases = accumulate_history(m_t_biases,g_t_biases, prev_factor=beta)\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "def update_theta_hat_adam(theta,beta,itr):\n",
        "    temp = []\n",
        "    for i in theta:\n",
        "        temp.append(i/(1-np.power(beta,itr+1)))\n",
        "\n",
        "    return temp\n",
        "\n",
        "def modify_deltas_adam(m_theta_hat, v_theta_hat, epsilon):\n",
        "    temp = []\n",
        "    for  i in range(len(m_theta_hat)):\n",
        "        temp.append(m_theta_hat[i]/(np.sqrt(v_theta_hat[i])+epsilon))\n",
        "\n",
        "    return temp\n",
        "\n",
        "\n",
        "def gradient_descent_adam_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta1, beta2, epsilon, loss_type=\"cross_entropy\"):\n",
        "\n",
        "    itr = 0\n",
        "    v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "    v_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function, loss_type)\n",
        "\n",
        "\n",
        "        m_weights = accumulate_history(m_weights,delta_Weights,prev_factor=beta1, current_factor=1-beta1)\n",
        "        m_biases = accumulate_history(m_biases,delta_Biases, prev_factor=beta1, current_factor=1-beta1)\n",
        "\n",
        "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta2, current_factor=1-beta2)\n",
        "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta2, current_factor=1-beta2)\n",
        "\n",
        "        m_w_hat = update_theta_hat_adam(m_weights,beta1,itr)\n",
        "        m_b_hat = update_theta_hat_adam(m_biases,beta1,itr)\n",
        "        v_w_hat = update_theta_hat_adam(v_t_weights,beta2,itr)\n",
        "        v_b_hat = update_theta_hat_adam(v_t_biases,beta2,itr)\n",
        "\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, modify_deltas_adam(m_w_hat, v_w_hat, epsilon), modify_deltas_adam(m_b_hat, v_b_hat, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "def modify_deltas_nadam(m_theta_hat, v_theta_hat, delta_theta, beta1, epsilon, itr):\n",
        "    factor = (1-beta1)/(1-np.power(beta1,itr+1))\n",
        "\n",
        "\n",
        "    w_t_temp = accumulate_history(m_theta_hat, delta_theta, prev_factor=beta1, current_factor=factor)\n",
        "\n",
        "    temp = []\n",
        "    for  i in range(len(m_theta_hat)):\n",
        "        temp.append(w_t_temp[i]/(np.sqrt(v_theta_hat[i])+epsilon))\n",
        "\n",
        "    return temp\n",
        "\n",
        "\n",
        "def gradient_descent_nadam_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta1, beta2, epsilon, loss_type=\"cross_entropy\"):\n",
        "\n",
        "    itr = 0\n",
        "    v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "    v_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function, loss_type)\n",
        "\n",
        "\n",
        "\n",
        "        m_weights = accumulate_history(m_weights,delta_Weights,prev_factor=beta1, current_factor=1-beta1)\n",
        "        m_biases = accumulate_history(m_biases,delta_Biases, prev_factor=beta1, current_factor=1-beta1)\n",
        "\n",
        "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta2, current_factor=1-beta2)\n",
        "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta2, current_factor=1-beta2)\n",
        "\n",
        "        m_w_hat = update_theta_hat_adam(m_weights,beta1,itr)\n",
        "        m_b_hat = update_theta_hat_adam(m_biases,beta1,itr)\n",
        "        v_w_hat = update_theta_hat_adam(v_t_weights,beta2,itr)\n",
        "        v_b_hat = update_theta_hat_adam(v_t_biases,beta2,itr)\n",
        "\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, modify_deltas_nadam(m_w_hat, v_w_hat, delta_Weights, beta1, epsilon, itr), modify_deltas_nadam(m_b_hat, v_b_hat, delta_Biases, beta1, epsilon, itr),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "#-------------------------------------------------------------------------------------------\n",
        "\n",
        "#training\n",
        "\n",
        "\n",
        "\n",
        "def train_model(X, Y, epochs=1, num_of_hidden_layers=1, size_of_layers=4, learning_rate=0.1, optimizer=\"sgd\", batch_size=4, l2_regularization_constant=0.001, weight_init_type=\"random\", activation_function=\"sigmoid\", beta=0, epsilon=1e-10, beta1=0, loss_type=\"cross_entropy\"):\n",
        "    run_name = \"{}_lr{}_bs{}_hl{}_hlsize{}_{}_{}_epochs{}_{}\".format(optimizer, learning_rate, batch_size, num_of_hidden_layers, size_of_layers, activation_function,weight_init_type, epochs,loss_type)\n",
        "    print(\"run name = {}\".format(run_name))\n",
        "    wandb.run.name=run_name\n",
        "\n",
        "    print(\"number of training datapoints:\",X.shape[0])\n",
        "    print(\"number of epochs:\", epochs)\n",
        "    print(\"number of hidden layers:\", num_of_hidden_layers)\n",
        "    print(\"size of hidden layers:\", size_of_layers)\n",
        "    print(\"learning rate:\", learning_rate)\n",
        "    print(\"optimizer:\", optimizer)\n",
        "    print(\"batch_size:\", batch_size)\n",
        "    print(\"l2 regularization constant:\", l2_regularization_constant)\n",
        "    print(\"weights and biases initialization type:\", weight_init_type)\n",
        "    print(\"activation function:\", activation_function)\n",
        "    print(\"beta1:\", beta)\n",
        "    print(\"beta2:\", beta1)\n",
        "    print(\"epsilon:\", epsilon)\n",
        "\n",
        "    if weight_init_type==\"random\":\n",
        "        initialize = random_initialize\n",
        "    else:\n",
        "        initialize = xavier_initialize\n",
        "\n",
        "    if optimizer==\"mini_batch\" or optimizer==\"sgd\":\n",
        "        gradient = gradient_descent_mini_batch_n\n",
        "    elif optimizer==\"mbgd\":\n",
        "        gradient = gradient_descent_momentum_based_n\n",
        "    elif optimizer==\"rmsprop\":\n",
        "        gradient = gradient_descent_RMSProp_n\n",
        "    elif optimizer==\"nagd\":\n",
        "        gradient = gradient_descent_nesterov_accelarated_n1\n",
        "    elif optimizer==\"adam\":\n",
        "        gradient = gradient_descent_adam_n\n",
        "    elif optimizer==\"nadam\":\n",
        "        gradient = gradient_descent_nadam_n\n",
        "    else:\n",
        "        gradient = gradient_descent_mini_batch_n\n",
        "\n",
        "    Weights, Biases = initialize(num_of_hidden_layers+2,size_of_layers,Y.shape[1], X.shape[1])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch number\", epoch+1, \" started\")\n",
        "        Weights, Biases = gradient(X, Y, learning_rate, num_of_hidden_layers+2, batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta1, epsilon, loss_type)\n",
        "        print(\"Epoch \",epoch+1, \" finished.\")\n",
        "        # Y_predict = []\n",
        "        # for i in validatex:\n",
        "        #     Y_predict.append(validate(i, Weights, Biases, activation_function))\n",
        "        Y_predict = validate_n(validatex,Weights,Biases,activation_function)\n",
        "        Y_predict_train = validate_n(X, Weights, Biases,activation_function)\n",
        "\n",
        "\n",
        "        if loss_type == \"mse\" or \"mean_squared_error\":\n",
        "            validation_loss = mean_squared_error( validatey,Y_predict)\n",
        "            training_loss = mean_squared_error( validatey,Y_predict)\n",
        "        else:\n",
        "            validation_loss = cross_entropy_loss( validatey,Y_predict)\n",
        "            training_loss = cross_entropy_loss(Y,Y_predict_train)\n",
        "\n",
        "        validation_accuracy = get_accuracy(validatey,Y_predict)\n",
        "        training_accuracy = get_accuracy(Y,Y_predict_train)\n",
        "\n",
        "        print(\"training accuracy after epoch \",epoch+1,\":\",training_accuracy,\"%\")\n",
        "        print(\"training Loss (cross entropy)\",training_loss)\n",
        "        print(\"validation accuracy after epoch \",epoch+1,\":\",validation_accuracy,\"%\")\n",
        "        print(\"Validation Loss (\",loss_type,\") : \",validation_loss)\n",
        "\n",
        "        wandb.log({'training_loss': training_loss, 'validation_loss': validation_loss, 'training_accuracy': training_accuracy, 'validation_accuracy': validation_accuracy, 'epoch_number': epoch+1})\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------\n",
        "\n",
        "# testing\n",
        "\n",
        "\n",
        "# def validate(X, Weights, Biases, activation_function):\n",
        "#     # some calculations\n",
        "#     H, A, Y_pred = forward_propagation(X, Weights=Weights, Biases=Biases, number_of_layers=len(Weights)+1, activation_function=activation_function)\n",
        "#     return Y_pred\n",
        "\n",
        "\n",
        "def validate_n(X, Weights, Biases, activation_function):\n",
        "    # some calculations\n",
        "    H, A, Y_pred = forward_propagation_n(X.transpose(), Weights=Weights, Biases=Biases, number_of_layers=len(Weights)+1, activation_function=activation_function, batch_size=X.shape[0])\n",
        "    return Y_pred.transpose()\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# loss\n",
        "\n",
        "def cross_entropy_loss( y_actual, y_pred):\n",
        "    loss = 0\n",
        "    for i in range(len(y_actual)):\n",
        "        loss = loss + (-np.log(y_pred[i][np.argmax(y_actual[i])] + 1e-10))\n",
        "    return loss/len(y_actual)\n",
        "\n",
        "def mean_squared_error( y_actual, y_pred):\n",
        "    loss = 0\n",
        "    for i in range(len(y_actual)):\n",
        "        for j in range(len(y_actual[i])):\n",
        "            loss = loss + (y_actual[i][j]-y_pred[i][j])**2\n",
        "    loss = loss / (len(y_actual)*len(y_actual[0]))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "e92087cc0731446786f5027745391ecc",
            "a0badda4ec8e4b5a87b14443e3690357",
            "f739f5dceb314de186c1c8898a3f8a36",
            "0378e640bba54b3b9bd6c8b9a86f8846",
            "ce51ccf3de004035a490f72eba2d5e74",
            "35d0bbd17cde4debb4556f11874455ed",
            "622d1a6925554fec8b65feaab96ec18b",
            "ec31894bb7ae4293912311401e17a4e6"
          ]
        },
        "id": "eQ66TEtIGIFC",
        "outputId": "02f187cd-5562-40a0-e330-e2d9e3276301"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"Dl_Assignment_1\")\n",
        "wandb.run.name=\"Confusion Matrix Test Data\"\n",
        "\n",
        "def plot_confusion_matrix( y_actual,y_pred):\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    y_actual = np.argmax(y_actual, axis=1)\n",
        "\n",
        "    label_display_name = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
        "                                                           y_true=y_actual,\n",
        "                                                           preds=y_pred,\n",
        "                                                           class_names=label_display_name,\n",
        "                                                           title=\"Confusion Matrix for Test Data\"\n",
        "                                                           )})\n",
        "plot_confusion_matrix(testy,Y_predict_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_TnAlNi55AY"
      },
      "outputs": [],
      "source": [
        "# Taking input data and normalizing data\n",
        "\n",
        "\n",
        "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "validateX = trainX[54000:]\n",
        "validateY = trainY[54000:]\n",
        "trainX = trainX[:54000]\n",
        "trainY = trainY[:54000]\n",
        "\n",
        "trainx = trainX.reshape(trainX.shape[0],-1)/255\n",
        "validatex = validateX.reshape(validateX.shape[0],-1)/255\n",
        "testx = testX.reshape(testX.shape[0],-1)/255\n",
        "\n",
        "# output dataset conversion One hot encoding\n",
        "import numpy as np\n",
        "trainy = [np.zeros(10) for i in range(trainX.shape[0])]\n",
        "validatey = [np.zeros(10) for i in range(validateX.shape[0])]\n",
        "testy = [np.zeros(10) for i in range(testX.shape[0])]\n",
        "\n",
        "for i in range(trainX.shape[0]):\n",
        "    trainy[i][trainY[i]] = 1\n",
        "\n",
        "for i in range(validateX.shape[0]):\n",
        "    validatey[i][validateY[i]] = 1\n",
        "\n",
        "for i in range(testX.shape[0]):\n",
        "    testy[i][testY[i]] = 1\n",
        "\n",
        "trainy = np.array(trainy)\n",
        "testy = np.array(testy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "g2PMQDnD55AZ",
        "outputId": "4a54ae7a-287a-46dc-ccff-93625f826457"
      },
      "outputs": [],
      "source": [
        "# Display few images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cnt = 0\n",
        "label_display_name = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "label = []\n",
        "\n",
        "for i in range(1, trainX.shape[0]):\n",
        "\n",
        "    if trainY[i] not in label:\n",
        "        cnt = cnt + 1\n",
        "        label.append(trainY[i])\n",
        "        plt.subplot(2, 5, cnt)\n",
        "        # Insert ith image with the color map 'grap'\n",
        "        plt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
        "        plt.title(label_display_name[trainY[i]])\n",
        "\n",
        "    if cnt == 10:\n",
        "        break\n",
        "\n",
        "# Display the entire plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOE3NP6I55AZ",
        "outputId": "edbcd1f3-558b-44eb-8aeb-268df43401b7"
      },
      "outputs": [],
      "source": [
        "## Training the model\n",
        "X = trainx\n",
        "Y = trainy\n",
        "epochs = 10\n",
        "num_of_hidden_layers = 3\n",
        "size_of_layers = 128\n",
        "learning_rate = 0.001\n",
        "optimizer = \"nadam\"\n",
        "batch_size = 150\n",
        "l2_regularization_constant = 0\n",
        "weight_init_type = \"xavier\"\n",
        "activation_function = \"tanh\"\n",
        "beta = 0.9\n",
        "beta1 = 0.999\n",
        "epsilon = 0.0001\n",
        "loss_type = \"mse\"\n",
        "\n",
        "\n",
        "\n",
        "Weights, Biases = train_model(\n",
        "                        X=X,\n",
        "                        Y=Y,\n",
        "                        epochs=epochs,\n",
        "                        num_of_hidden_layers=num_of_hidden_layers,\n",
        "                        size_of_layers=size_of_layers,\n",
        "                        learning_rate=learning_rate,\n",
        "                        optimizer=optimizer,\n",
        "                        batch_size=batch_size,\n",
        "                        l2_regularization_constant=l2_regularization_constant,\n",
        "                        weight_init_type=weight_init_type,\n",
        "                        activation_function=activation_function,\n",
        "                        beta=beta,\n",
        "                        beta1=beta1,\n",
        "                        epsilon=epsilon\n",
        "                    )\n",
        "\n",
        "Y_predict_test = validate_n(testx,Weights,Biases,activation_function)\n",
        "print(\"accuracy test : \",get_accuracy(testy,Y_predict_test ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt7Au3sg5iDm",
        "outputId": "9976356b-ac1d-4180-c7c6-4d7f03effa43"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFlVvn6A5p62"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_QGtmhA5vqQ",
        "outputId": "302dd3b6-7858-483e-e4a4-d62739973b8d"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c906704b229648e4a351732716d7789e",
            "1635f25c3d75471f838ce9c828f205b3",
            "2f2bd0f4dbff4ccf9ce42425bdd3a08e",
            "3eef49f831c44e718162ad41b37be096",
            "d48ebde8d7f84c6da0ac497b88cc1f66",
            "8bee2b89410e42539465eb50a292f4b2",
            "6ce9bec6aacf4ac3b772c1acfc830b64",
            "de6a5836604b4421bdb0742ce07db131"
          ]
        },
        "id": "WdC8juaHfflh",
        "outputId": "a6416ab5-3ab8-408f-d74c-c42b7ccb7528"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    wandb.init(project='DL_Assignment_1')\n",
        "    config=wandb.config\n",
        "    X = trainx\n",
        "    Y = trainy\n",
        "    epochs = config.epochs\n",
        "    num_of_hidden_layers = config.num_of_hidden_layer\n",
        "    size_of_layers = config.size_of_layer\n",
        "    learning_rate = config.learning_rate\n",
        "    optimizer = config.optimizer\n",
        "    batch_size = config.batch_size\n",
        "    l2_regularization_constant = config.l2_reg_constant\n",
        "    weight_init_type = config.weight_init_type\n",
        "    activation_function = config.activation_function\n",
        "    beta = 0.9\n",
        "    beta1 = 0.999\n",
        "    epsilon = 0.0001\n",
        "    loss_type = config.loss_type # take from command line argument\n",
        "\n",
        "    Weights, Biases = train_model(\n",
        "                        X=X,\n",
        "                        Y=Y,\n",
        "                        epochs=epochs,\n",
        "                        num_of_hidden_layers=num_of_hidden_layers,\n",
        "                        size_of_layers=size_of_layers,\n",
        "                        learning_rate=learning_rate,\n",
        "                        optimizer=optimizer,\n",
        "                        batch_size=batch_size,\n",
        "                        l2_regularization_constant=l2_regularization_constant,\n",
        "                        weight_init_type=weight_init_type,\n",
        "                        activation_function=activation_function,\n",
        "                        beta=beta,\n",
        "                        beta1=beta1,\n",
        "                        epsilon=epsilon,\n",
        "                        loss_type=loss_type\n",
        "                    )\n",
        "\n",
        "\n",
        "# Define the sweep configuration\n",
        "sweep_config_old = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'Accuracy per Epoch',\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [0.001, 0.005]\n",
        "        },\n",
        "        'activation_function': {\n",
        "            'values': ['sigmoid']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [50]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [5]\n",
        "        },\n",
        "        'num_of_hidden_layer': {\n",
        "            'values': [4]\n",
        "        },\n",
        "        'size_of_layer': {\n",
        "            'values': [32]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['mbgd']\n",
        "        },\n",
        "        'weight_init_type': {\n",
        "            'values': ['random','xavier']\n",
        "        },\n",
        "        'l2_reg_constant': {\n",
        "            'values' : [0.0005]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'Mean_Squared_Error',\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "        },\n",
        "        'activation_function': {\n",
        "            'values': ['sigmoid', 'relu', 'tanh']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [25, 50, 100, 150, 200]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'num_of_hidden_layer': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'size_of_layer': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'mbgd', 'nagd', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'weight_init_type': {\n",
        "            'values': ['random', 'xavier']\n",
        "        },\n",
        "        'l2_reg_constant': {\n",
        "            'values' : [0, 0.0005, 0.5]\n",
        "        },\n",
        "        'loss_type': {\n",
        "            'values' : [\"mse\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assignment_1')\n",
        "\n",
        "wandb.agent(sweep_id, function=main, count = 1)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWG_IUy0AZuN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hat4ZgDSAlwI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLKeoaH9Al08"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY42SROQ55AZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4KhgULb2WBg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD4z-nbs2Jgt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XbcKC7i2Nr3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2ZnCVm-6VTX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kCBFyuCK1E0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0378e640bba54b3b9bd6c8b9a86f8846": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1635f25c3d75471f838ce9c828f205b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d48ebde8d7f84c6da0ac497b88cc1f66",
            "placeholder": "",
            "style": "IPY_MODEL_8bee2b89410e42539465eb50a292f4b2",
            "value": "0.015 MB of 0.015 MB uploaded\r"
          }
        },
        "2f2bd0f4dbff4ccf9ce42425bdd3a08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce9bec6aacf4ac3b772c1acfc830b64",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de6a5836604b4421bdb0742ce07db131",
            "value": 1
          }
        },
        "35d0bbd17cde4debb4556f11874455ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eef49f831c44e718162ad41b37be096": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622d1a6925554fec8b65feaab96ec18b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ce9bec6aacf4ac3b772c1acfc830b64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bee2b89410e42539465eb50a292f4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0badda4ec8e4b5a87b14443e3690357": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce51ccf3de004035a490f72eba2d5e74",
            "placeholder": "",
            "style": "IPY_MODEL_35d0bbd17cde4debb4556f11874455ed",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "c906704b229648e4a351732716d7789e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1635f25c3d75471f838ce9c828f205b3",
              "IPY_MODEL_2f2bd0f4dbff4ccf9ce42425bdd3a08e"
            ],
            "layout": "IPY_MODEL_3eef49f831c44e718162ad41b37be096"
          }
        },
        "ce51ccf3de004035a490f72eba2d5e74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d48ebde8d7f84c6da0ac497b88cc1f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6a5836604b4421bdb0742ce07db131": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e92087cc0731446786f5027745391ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0badda4ec8e4b5a87b14443e3690357",
              "IPY_MODEL_f739f5dceb314de186c1c8898a3f8a36"
            ],
            "layout": "IPY_MODEL_0378e640bba54b3b9bd6c8b9a86f8846"
          }
        },
        "ec31894bb7ae4293912311401e17a4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f739f5dceb314de186c1c8898a3f8a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_622d1a6925554fec8b65feaab96ec18b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec31894bb7ae4293912311401e17a4e6",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
