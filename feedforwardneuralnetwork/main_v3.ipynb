{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sR9Ghl_O55AV"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sbqIOG0i55AX"
      },
      "outputs": [],
      "source": [
        "# All functions required\n",
        "\n",
        "# Activation functions\n",
        "# 1 Relu\n",
        "def relu(a):\n",
        "    return np.max(a,0)\n",
        "\n",
        "def relu_vector_old(a):\n",
        "    temp = []\n",
        "    for i in range(len(a)):\n",
        "        temp.append(relu(a[i]))\n",
        "    return temp\n",
        "\n",
        "def relu_vector(a):\n",
        "    return np.maximum(a,0)\n",
        "\n",
        "\n",
        "# 2 Sigmoid\n",
        "def sigmoid(a):\n",
        "    ans = 1/(1+np.exp(-a))\n",
        "    return ans\n",
        "\n",
        "def sigmoid_vector_old(a):\n",
        "    temp=[]\n",
        "    for i in range(len(a)):\n",
        "        temp.append(sigmoid(a[i]))\n",
        "    return temp\n",
        "\n",
        "def sigmoid_vector(a):\n",
        "    a = np.clip(a, -200,200)\n",
        "    ans = 1/(1+np.exp(-a))\n",
        "    return ans\n",
        "\n",
        "# 3 Tanh\n",
        "def tanh(a):\n",
        "    a = np.clip(a, -200, 200)\n",
        "    ans = (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
        "    return ans\n",
        "\n",
        "def tanh_vector_old(a):\n",
        "    temp = []\n",
        "    for i in range(len(a)):\n",
        "        temp.append(tanh(a[i]))\n",
        "    return temp\n",
        "\n",
        "def tanh_vector(a):\n",
        "    return np.tanh(a)\n",
        "    # a = np.clip(a, -200, 200)\n",
        "    # ans = (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
        "    # return ans\n",
        "\n",
        "# Output Activation Function\n",
        "\n",
        "# Softmax\n",
        "def softmax(a):\n",
        "    a = np.clip(a, -200, 200)\n",
        "    return np.exp(a)/np.sum(np.exp(a))\n",
        "\n",
        "#-------------------------------------------------------\n",
        "# utility functions\n",
        "\n",
        "def hadamard_product(A,B):\n",
        "    # result = []\n",
        "    # for i in range(len(A)):\n",
        "    #     result.append(A[i]*B[i])\n",
        "    return np.multiply(np.array(A),np.array(B))\n",
        "    # return result\n",
        "\n",
        "def random_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
        "    if number_of_layers<=2:\n",
        "        return [np.random.randn(nodes_in_output_layer,input_layer_size)], [np.random.randn(nodes_in_output_layer)]\n",
        "\n",
        "    if number_of_layers==3:\n",
        "        Weights = [np.random.randn(nodes_per_hidden_layer, input_layer_size), np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)]\n",
        "        Biases = [np.random.randn(nodes_per_hidden_layer), np.random.randn(nodes_in_output_layer)]\n",
        "        return Weights, Biases\n",
        "\n",
        "    WS = np.random.randn(nodes_per_hidden_layer, input_layer_size)*0.1\n",
        "    W = np.random.randn(number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer)*0.1\n",
        "    B = np.random.randn(number_of_layers-2, nodes_per_hidden_layer)*0.1\n",
        "    WL = np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)*0.1\n",
        "    BL = np.random.randn(nodes_in_output_layer)*0.1\n",
        "\n",
        "    Weights = [WS] + [i for i in W] + [WL]\n",
        "    Biases = [i for i in B] + [BL]\n",
        "    return Weights, Biases\n",
        "\n",
        "def xavier_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
        "    if number_of_layers<=2:\n",
        "        return [np.random.randn(nodes_in_output_layer,input_layer_size)], [np.random.randn(nodes_in_output_layer)]\n",
        "\n",
        "    fact_in = np.sqrt(6/(input_layer_size + nodes_per_hidden_layer))\n",
        "    fact_out = np.sqrt(6/(nodes_in_output_layer + nodes_per_hidden_layer))\n",
        "    fact_hid = np.sqrt(6/(nodes_per_hidden_layer + nodes_per_hidden_layer))\n",
        "\n",
        "    if number_of_layers==3:\n",
        "        # fact_in = np.sqrt(6/(input_layer_size + nodes_per_hidden_layer))\n",
        "        # fact_out = np.sqrt(6/(nodes_in_output_layer + nodes_per_hidden_layer))\n",
        "        Weights = [np.random.uniform(-fact_in, fact_in, (nodes_per_hidden_layer, input_layer_size)), np.random.uniform(-fact_out,fact_out,(nodes_in_output_layer, nodes_per_hidden_layer))]\n",
        "        Biases = [np.zeros(nodes_per_hidden_layer), np.zeros(nodes_in_output_layer)]\n",
        "        return Weights, Biases\n",
        "\n",
        "    WS = np.random.uniform(-fact_in, fact_in, (nodes_per_hidden_layer, input_layer_size))\n",
        "    W = np.random.uniform(-fact_hid, fact_hid, (number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer))\n",
        "    B = np.zeros([number_of_layers-2, nodes_per_hidden_layer])\n",
        "    WL = np.random.uniform(-fact_out,fact_out,(nodes_in_output_layer, nodes_per_hidden_layer))\n",
        "    BL = np.zeros(nodes_in_output_layer)\n",
        "\n",
        "    Weights = [WS] + [i for i in W] + [WL]\n",
        "    Biases = [i for i in B] + [BL]\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_accuracy(Y_actual, Y_predicted):\n",
        "    total = len(Y_actual)\n",
        "    print(\"total data points in validation \",total)\n",
        "    cnt = 0\n",
        "    for i in range(total):\n",
        "        if np.argmax(Y_actual[i]) == np.argmax(Y_predicted[i]):\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    return (cnt/total)*100\n",
        "\n",
        "\n",
        "def get_average_delta_WandB(delta_W_acc, delta_B_acc):\n",
        "    for i in range(1,len(delta_W_acc)):\n",
        "        for j in range(len(delta_W_acc[0])):\n",
        "            delta_W_acc[0][j] = np.add(delta_W_acc[0][j] , delta_W_acc[i][j])\n",
        "            # if i==len(delta_W_acc)-1:\n",
        "            #     delta_W_acc[0][j] = delta_W_acc[0][j] / len(delta_W_acc)\n",
        "\n",
        "        for j in range(len(delta_B_acc[0])):\n",
        "            delta_B_acc[0][j] = np.add(delta_B_acc[0][j] , delta_B_acc[i][j])\n",
        "            # if i==len(delta_B_acc)-1:\n",
        "            #     delta_B_acc[0][j] = delta_B_acc[0][j] / len(delta_B_acc)\n",
        "\n",
        "    return delta_W_acc[0], delta_B_acc[0]\n",
        "\n",
        "\n",
        "#---------------------------------------------------\n",
        "\n",
        "# Derivative functions\n",
        "\n",
        "def derivative_sigmoid(a):\n",
        "    return sigmoid(a) * (1-sigmoid(a))\n",
        "\n",
        "def derivative_tanh(a):\n",
        "    return 1 - (tanh(a)**2)\n",
        "\n",
        "def derivative_relu(a):\n",
        "    # if a<=0:\n",
        "    #     return 0\n",
        "    a[a<=0] = 0\n",
        "    a[a>0] = 1\n",
        "    return a\n",
        "\n",
        "\n",
        "#---------------------------------------------------\n",
        "# Pre Avtivation function\n",
        "\n",
        "\n",
        "def pre_activation(W, h, b):\n",
        "    # print(W.shape)\n",
        "    # print(h.shape)\n",
        "    # print(b.shape)\n",
        "    return np.add(np.matmul(W,h) , b)\n",
        "\n",
        "\n",
        "#------------------------------------------------------\n",
        "\n",
        "# forward propagation\n",
        "\n",
        "\n",
        "def forward_propagation(X, Weights, Biases, number_of_layers, activation_function):\n",
        "\n",
        "    if activation_function==\"relu\":\n",
        "        activation = relu_vector\n",
        "    elif activation_function==\"tanh\":\n",
        "        activation = tanh_vector\n",
        "    else:\n",
        "        activation = sigmoid_vector\n",
        "\n",
        "    A = []\n",
        "    H = [X]\n",
        "    for i in range(number_of_layers-2):\n",
        "        A.append(pre_activation(Weights[i],H[i],Biases[i]))\n",
        "        H.append(activation(A[i]))\n",
        "\n",
        "    A.append(pre_activation(Weights[-1], H[-1], Biases[-1]))\n",
        "\n",
        "    y_pred = softmax(A[-1])\n",
        "\n",
        "    return H, A, y_pred\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "# Backward Propagation\n",
        "\n",
        "def backward_propagation(H, A, W, y_actual, y_pred, number_of_layers, activation_function):\n",
        "\n",
        "    if activation_function==\"relu\":\n",
        "        derivative = derivative_relu\n",
        "    elif activation_function==\"tanh\":\n",
        "        derivative = derivative_tanh\n",
        "    else:\n",
        "        derivative = derivative_sigmoid\n",
        "    # delta_A = [0 for i in range(number_of_layers-1)]\n",
        "    delta_W = [0 for i in range(number_of_layers-1)]\n",
        "    delta_B = [0 for i in range(number_of_layers-1)]\n",
        "    # delta_H = [0 for i in range(number_of_layers-2)]\n",
        "\n",
        "    # gradient with respect to output\n",
        "    # delta_A[-1] = -(y_actual-y_pred)\n",
        "    delta_A = -(y_actual-y_pred)\n",
        "    delta_H = None\n",
        "\n",
        "\n",
        "    for k in reversed(range(number_of_layers-1)):\n",
        "\n",
        "        # gradient with respect to parameters\n",
        "        # delta_W[k] = np.outer(delta_A[k],H[k-1])\n",
        "        delta_W[k] = np.outer(delta_A, H[k])\n",
        "        # delta_B[k] = delta_A[k]\n",
        "        delta_B[k] = delta_A\n",
        "\n",
        "        if k==0:\n",
        "            break\n",
        "        # gradient with respect to layer below\n",
        "        # delta_H[k-1] = np.matmul(W[k].transpose() , delta_A[k])\n",
        "        delta_H = np.matmul(W[k].transpose() , delta_A)\n",
        "\n",
        "        #gradient with respect to layer below (i.e. pre-activation)\n",
        "        # delta_A[k-1] = hadamard_product(delta_H[k-1],[derivative(i) for i in A[k-1]])\n",
        "        delta_A = hadamard_product(delta_H,[derivative(i) for i in A[k-1]])\n",
        "\n",
        "\n",
        "    return delta_W, delta_B\n",
        "\n",
        "\n",
        "#-------------------------------------------\n",
        "\n",
        "# new optimization\n",
        "\n",
        "\n",
        "def forward_propagation_n(X, Weights, Biases, number_of_layers, activation_function, batch_size):\n",
        "\n",
        "    if activation_function==\"relu\":\n",
        "        activation = relu_vector\n",
        "    elif activation_function==\"tanh\":\n",
        "        activation = tanh_vector\n",
        "    else:\n",
        "        activation = sigmoid_vector\n",
        "\n",
        "    A = []\n",
        "    H = [X]\n",
        "    for i in range(number_of_layers-2):\n",
        "\n",
        "        modified_bias = Biases[i].reshape(1,-1)\n",
        "        modified_bias_N = np.repeat(modified_bias, batch_size, axis=0).transpose()\n",
        "\n",
        "        A.append(pre_activation(Weights[i],H[i],modified_bias_N))\n",
        "        H.append(activation(A[i]))\n",
        "\n",
        "    modified_bias = Biases[-1].reshape(1,-1)\n",
        "    modified_bias_N = np.repeat(modified_bias, batch_size, axis=0).transpose()\n",
        "\n",
        "    A.append(pre_activation(Weights[-1], H[-1], modified_bias_N))\n",
        "\n",
        "    y_pred_temp = []\n",
        "    A_trns = A[-1].transpose()\n",
        "    for i in range(batch_size):\n",
        "        y_pred_temp.append(softmax(A_trns[i]))\n",
        "\n",
        "    y_pred = np.array(y_pred_temp).transpose()\n",
        "\n",
        "    return H, A, y_pred\n",
        "\n",
        "\n",
        "# Backward Propagation\n",
        "\n",
        "def backward_propagation_n(H, A, W, y_actual, y_pred, number_of_layers, activation_function):\n",
        "\n",
        "    if activation_function==\"relu\":\n",
        "        derivative = derivative_relu\n",
        "    elif activation_function==\"tanh\":\n",
        "        derivative = derivative_tanh\n",
        "    else:\n",
        "        derivative = derivative_sigmoid\n",
        "    # delta_A = [0 for i in range(number_of_layers-1)]\n",
        "    delta_W = [0 for i in range(number_of_layers-1)]\n",
        "    delta_B = [0 for i in range(number_of_layers-1)]\n",
        "    # delta_H = [0 for i in range(number_of_layers-2)]\n",
        "\n",
        "    # gradient with respect to output\n",
        "    # delta_A[-1] = -(y_actual-y_pred)\n",
        "    delta_A = -(y_actual-y_pred)\n",
        "    delta_H = None\n",
        "\n",
        "\n",
        "    for k in reversed(range(number_of_layers-1)):\n",
        "\n",
        "        # gradient with respect to parameters\n",
        "        # delta_W[k] = np.outer(delta_A[k],H[k])\n",
        "        delta_W[k] = np.matmul(delta_A, H[k].transpose())\n",
        "        # delta_B[k] = delta_A[k]\n",
        "        delta_B[k] = np.sum(delta_A,axis=1)\n",
        "\n",
        "        if k==0:\n",
        "            break\n",
        "        # gradient with respect to layer below\n",
        "        # delta_H[k-1] = np.matmul(W[k].transpose() , delta_A[k])\n",
        "        delta_H = np.matmul(W[k].transpose() , delta_A)\n",
        "\n",
        "        #gradient with respect to layer below (i.e. pre-activation)\n",
        "        # delta_A[k-1] = hadamard_product(delta_H[k-1],[derivative(i) for i in A[k-1]])\n",
        "        delta_A = hadamard_product(delta_H,[derivative(i) for i in A[k-1]])\n",
        "\n",
        "        # delta_B[k] = np.sum(delta_B[k],axis=0)\n",
        "\n",
        "\n",
        "    return delta_W, delta_B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "\n",
        "\n",
        "# gradient descent algorithms\n",
        "\n",
        "\n",
        "def update_weights_and_biases_n(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant):\n",
        "\n",
        "    for i in range(len(Weights)):\n",
        "        # print(\"+++++++++++++++++++++++++++\")\n",
        "        # print(Biases[i].shape)\n",
        "        # print(delta_Biases[i].shape)\n",
        "        # Weights[i] = Weights[i] - learning_rate * delta_Weights[i]\n",
        "        # Biases[i] = Biases[i] - learning_rate * delta_Biases[i]\n",
        "        for j in range(len(Weights[i])):\n",
        "            Weights[i][j] = Weights[i][j] - learning_rate * delta_Weights[i][j] - (learning_rate * l2_regularization_constant * Weights[i][j])\n",
        "\n",
        "        for j in range(len(Biases[i])):\n",
        "            Biases[i][j] = Biases[i][j] - learning_rate * delta_Biases[i][j] #- (learning_rate * l2_regularization_constant * Biases[i][j])\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def update_weights_and_biases(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant):\n",
        "    for i in range(len(Weights)):\n",
        "        # Weights[i] = Weights[i] - learning_rate * delta_Weights[i]\n",
        "        # Biases[i] = Biases[i] - learning_rate * delta_Biases[i]\n",
        "        for j in range(len(Weights[i])):\n",
        "            Weights[i][j] = Weights[i][j] - learning_rate * delta_Weights[i][j] - (learning_rate * l2_regularization_constant * Weights[i][j])\n",
        "\n",
        "        for j in range(len(Biases[i])):\n",
        "            Biases[i][j] = Biases[i][j] - learning_rate * delta_Biases[i][j] #- (learning_rate * l2_regularization_constant * Biases[i][j])\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_stochastic(X, Y, learning_rate, number_of_layers, batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, epsilon=0):\n",
        "\n",
        "    # Weights, Biases = random_initialize(number_of_layers,nodes_per_hidden_layer,nodes_in_output_layer)\n",
        "    # itr = 0\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0])):\n",
        "        # H, A, y_pred = forward_propagation(X[itr*batch_size:(itr+1)*batch_size], Weights, Biases, number_of_layers)\n",
        "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "        # return None, None\n",
        "        # delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size], y_pred, number_of_layers)\n",
        "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "        Weights , Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant)\n",
        "        # itr = itr + 1\n",
        "    return Weights, Biases\n",
        "\n",
        "def gradient_descent_mini_batch(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, epsilon=0):\n",
        "    itr = 0\n",
        "    delta_W_acc = []\n",
        "    delta_B_acc = []\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0])):\n",
        "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "\n",
        "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "\n",
        "        delta_W_acc.append(delta_Weights)\n",
        "        delta_B_acc.append(delta_Biases)\n",
        "\n",
        "        # itr = itr + 1\n",
        "        if (itr+1)%batch_size==0:\n",
        "            delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
        "\n",
        "            delta_W_acc = []\n",
        "            delta_B_acc = []\n",
        "            delta_W_avg = 0\n",
        "            delta_B_avg = 0\n",
        "\n",
        "\n",
        "    if delta_B_acc and delta_W_acc:\n",
        "        delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_mini_batch_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, beta1=0, epsilon=0):\n",
        "    itr = 0\n",
        "    # delta_W_acc = []\n",
        "    # delta_B_acc = []\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function, batch_size)\n",
        "\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "\n",
        "        # delta_W_acc.append(delta_Weights)\n",
        "        # delta_B_acc.append(delta_Biases)\n",
        "\n",
        "        # itr = itr + 1\n",
        "        # if (itr+1)%batch_size==0:\n",
        "        # delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant)\n",
        "\n",
        "        # delta_W_acc = []\n",
        "        # delta_B_acc = []\n",
        "        # delta_W_avg = 0\n",
        "        # delta_B_avg = 0\n",
        "\n",
        "    # if X.shape[0]-itr\n",
        "\n",
        "    # if delta_B_acc and delta_W_acc:\n",
        "    #     delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
        "\n",
        "        # Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "def accumulate_history(prev, current, prev_factor=1, current_factor=1):\n",
        "    temp = []\n",
        "    for i in range(len(prev)):\n",
        "        temp.append((prev[i]*prev_factor) + (current[i]*current_factor))\n",
        "\n",
        "    return temp\n",
        "\n",
        "def gradient_descent_momentum_based(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon=0):\n",
        "\n",
        "    itr = 0\n",
        "    u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    # u_t_list = [u_t]\n",
        "    for itr in tqdm(range(X.shape[0])):\n",
        "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "        # u_t = beta * u_t + delta_Weights\n",
        "        u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "        u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "        # u_t_list.append()\n",
        "        # itr = itr + 1\n",
        "\n",
        "        if (itr+1)%batch_size==0:\n",
        "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "    if itr%batch_size!=0:\n",
        "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_momentum_based_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon=0):\n",
        "\n",
        "    itr = 0\n",
        "    u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    # u_t_list = [u_t]\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "        # u_t = beta * u_t + delta_Weights\n",
        "        u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "        u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "        # u_t_list.append()\n",
        "        # itr = itr + 1\n",
        "\n",
        "        # if (itr+1)%batch_size==0:\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "    # if itr%batch_size!=0:\n",
        "    #     Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def square_each_term(a):\n",
        "    temp = []\n",
        "    for i in range(len(a)):\n",
        "        temp.append(np.array(a[i])**2)\n",
        "    return temp\n",
        "\n",
        "def modify_deltas_RMSProp(v_t, w_t, epsilon):\n",
        "    temp = []\n",
        "    for i in range(len(v_t)):\n",
        "        temp.append(w_t[i] / (np.sqrt(v_t[i]) + epsilon))\n",
        "    return temp\n",
        "\n",
        "def gradient_descent_RMSProp(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon):\n",
        "\n",
        "    itr = 0\n",
        "    v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0])):\n",
        "\n",
        "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
        "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
        "\n",
        "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta, current_factor=1-beta)\n",
        "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta, current_factor=1-beta)\n",
        "\n",
        "        if (itr+1)%batch_size==0:\n",
        "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    if itr%batch_size!=0:\n",
        "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_RMSProp_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon):\n",
        "\n",
        "    itr = 0\n",
        "    v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "\n",
        "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta, current_factor=1-beta)\n",
        "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta, current_factor=1-beta)\n",
        "\n",
        "        # if (itr+1)%batch_size==0:\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    # if itr%batch_size!=0:\n",
        "    #     Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def modify_W_B_NAGD(u_t, w_t, beta):\n",
        "    temp = []\n",
        "    for i in range(len(u_t)):\n",
        "        temp.append(w_t[i]- (beta*u_t[i]))\n",
        "    return temp\n",
        "\n",
        "\n",
        "def gradient_descent_nesterov_accelarated(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon=0):\n",
        "    itr = 0\n",
        "    u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    # u_t_list = [u_t]\n",
        "    for itr in tqdm(range(X.shape[0])):\n",
        "        H, A, y_pred = forward_propagation(X[itr], modify_W_B_NAGD(u_t_weights, Weights, beta), modify_W_B_NAGD(u_t_biases, Biases, beta), number_of_layers, activation_function)\n",
        "        delta_Weights, delta_Biases = backward_propagation(H, A, modify_W_B_NAGD(u_t_weights, Weights, beta), Y[itr], y_pred, number_of_layers, activation_function)\n",
        "        # u_t = beta * u_t + delta_Weights\n",
        "        u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "        u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "        # u_t_list.append()\n",
        "        # itr = itr + 1\n",
        "\n",
        "        if (itr+1)%batch_size==0:\n",
        "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "    if itr%batch_size!=0:\n",
        "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "def gradient_descent_nesterov_accelarated_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon=0):\n",
        "    itr = 0\n",
        "    u_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    u_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), modify_W_B_NAGD(u_t_weights, Weights, beta), modify_W_B_NAGD(u_t_biases, Biases, beta), number_of_layers, activation_function, batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "\n",
        "        u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
        "        u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "def gradient_descent_nesterov_accelarated_n1(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta2, epsilon=0):\n",
        "    itr = 0\n",
        "    g_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    g_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function, batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "\n",
        "\n",
        "        g_t_weights = delta_Weights\n",
        "        g_t_biases = delta_Biases\n",
        "\n",
        "        # m_t_weights = beta * m_t_weights + g_t_weights\n",
        "        # m_t_biases = beta * m_t_biases + g_t_biases\n",
        "\n",
        "        m_t_weights = accumulate_history(m_t_weights,g_t_weights,prev_factor=beta)\n",
        "        m_t_biases = accumulate_history(m_t_biases,g_t_biases, prev_factor=beta)\n",
        "\n",
        "\n",
        "        u_t_weights = accumulate_history(m_t_weights,g_t_weights,prev_factor=beta)\n",
        "        u_t_biases = accumulate_history(m_t_biases,g_t_biases, prev_factor=beta)\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "def update_theta_hat_adam(theta,beta,itr):\n",
        "    temp = []\n",
        "    for i in theta:\n",
        "        temp.append(i/(1-np.power(beta,itr+1)))\n",
        "\n",
        "    return temp\n",
        "\n",
        "def modify_deltas_adam(m_theta_hat, v_theta_hat, epsilon):\n",
        "    temp = []\n",
        "    for  i in range(len(m_theta_hat)):\n",
        "        temp.append(m_theta_hat[i]/(np.sqrt(v_theta_hat[i])+epsilon))\n",
        "\n",
        "    return temp\n",
        "\n",
        "\n",
        "def gradient_descent_adam_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta1, beta2, epsilon):\n",
        "\n",
        "    itr = 0\n",
        "    v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "    v_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "\n",
        "\n",
        "        m_weights = accumulate_history(m_weights,delta_Weights,prev_factor=beta1, current_factor=1-beta1)\n",
        "        m_biases = accumulate_history(m_biases,delta_Biases, prev_factor=beta1, current_factor=1-beta1)\n",
        "\n",
        "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta2, current_factor=1-beta2)\n",
        "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta2, current_factor=1-beta2)\n",
        "\n",
        "        m_w_hat = update_theta_hat_adam(m_weights,beta1,itr)\n",
        "        m_b_hat = update_theta_hat_adam(m_biases,beta1,itr)\n",
        "        v_w_hat = update_theta_hat_adam(v_t_weights,beta2,itr)\n",
        "        v_b_hat = update_theta_hat_adam(v_t_biases,beta2,itr)\n",
        "\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, modify_deltas_adam(m_w_hat, v_w_hat, epsilon), modify_deltas_adam(m_b_hat, v_b_hat, epsilon),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "def modify_deltas_nadam(m_theta_hat, v_theta_hat, delta_theta, beta1, epsilon, itr):\n",
        "    factor = (1-beta1)/(1-np.power(beta1,itr+1))\n",
        "\n",
        "\n",
        "    w_t_temp = accumulate_history(m_theta_hat, delta_theta, prev_factor=beta1, current_factor=factor)\n",
        "\n",
        "    temp = []\n",
        "    for  i in range(len(m_theta_hat)):\n",
        "        temp.append(w_t_temp[i]/(np.sqrt(v_theta_hat[i])+epsilon))\n",
        "\n",
        "    return temp\n",
        "\n",
        "\n",
        "def gradient_descent_nadam_n(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta1, beta2, epsilon):\n",
        "\n",
        "    itr = 0\n",
        "    v_t_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_t_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_weights = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_biases = [np.zeros_like(bias) for bias in Biases]\n",
        "    m_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    m_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "    v_w_hat = [np.zeros_like(weight) for weight in Weights]\n",
        "    v_b_hat = [np.zeros_like(bias) for bias in Biases]\n",
        "\n",
        "    for itr in tqdm(range(X.shape[0]//batch_size)):\n",
        "\n",
        "        H, A, y_pred = forward_propagation_n(X[itr*batch_size:(itr+1)*batch_size].transpose(), Weights, Biases, number_of_layers, activation_function,batch_size)\n",
        "        delta_Weights, delta_Biases = backward_propagation_n(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size].transpose(), y_pred, number_of_layers, activation_function)\n",
        "\n",
        "\n",
        "\n",
        "        m_weights = accumulate_history(m_weights,delta_Weights,prev_factor=beta1, current_factor=1-beta1)\n",
        "        m_biases = accumulate_history(m_biases,delta_Biases, prev_factor=beta1, current_factor=1-beta1)\n",
        "\n",
        "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta2, current_factor=1-beta2)\n",
        "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta2, current_factor=1-beta2)\n",
        "\n",
        "        m_w_hat = update_theta_hat_adam(m_weights,beta1,itr)\n",
        "        m_b_hat = update_theta_hat_adam(m_biases,beta1,itr)\n",
        "        v_w_hat = update_theta_hat_adam(v_t_weights,beta2,itr)\n",
        "        v_b_hat = update_theta_hat_adam(v_t_biases,beta2,itr)\n",
        "\n",
        "\n",
        "        Weights, Biases = update_weights_and_biases_n(learning_rate, Weights, Biases, modify_deltas_nadam(m_w_hat, v_w_hat, delta_Weights, beta1, epsilon, itr), modify_deltas_nadam(m_b_hat, v_b_hat, delta_Biases, beta1, epsilon, itr),l2_regularization_constant)\n",
        "\n",
        "\n",
        "    return Weights, Biases\n",
        "#-------------------------------------------------------------------------------------------\n",
        "\n",
        "#training\n",
        "\n",
        "\n",
        "\n",
        "def train_model(X, Y, epochs=1, num_of_hidden_layers=1, size_of_layers=4, learning_rate=0.1, optimizer=\"sgd\", batch_size=4, l2_regularization_constant=0.001, weight_init_type=\"random\", activation_function=\"sigmoid\", beta=0, epsilon=1e-10, beta1=0):\n",
        "\n",
        "    print(\"number of training datapoints:\",X.shape[0])\n",
        "    print(\"number of epochs:\", epochs)\n",
        "    print(\"number of hidden layers:\", num_of_hidden_layers)\n",
        "    print(\"size of hidden layers:\", size_of_layers)\n",
        "    print(\"learning rate:\", learning_rate)\n",
        "    print(\"optimizer:\", optimizer)\n",
        "    print(\"batch_size:\", batch_size)\n",
        "    print(\"l2 regularization constant:\", l2_regularization_constant)\n",
        "    print(\"weights and biases initialization type:\", weight_init_type)\n",
        "    print(\"activation function:\", activation_function)\n",
        "    print(\"beta1:\", beta)\n",
        "    print(\"beta2:\", beta1)\n",
        "    print(\"epsilon:\", epsilon)\n",
        "\n",
        "    if weight_init_type==\"random\":\n",
        "        initialize = random_initialize\n",
        "    else:\n",
        "        initialize = xavier_initialize\n",
        "\n",
        "    if optimizer==\"mini_batch\":\n",
        "        gradient = gradient_descent_mini_batch_n\n",
        "    elif optimizer==\"mbgd\":\n",
        "        gradient = gradient_descent_momentum_based_n\n",
        "    elif optimizer==\"rmsprop\":\n",
        "        gradient = gradient_descent_RMSProp_n\n",
        "    elif optimizer==\"nagd\":\n",
        "        gradient = gradient_descent_nesterov_accelarated_n1\n",
        "    elif optimizer==\"adam\":\n",
        "        gradient = gradient_descent_adam_n\n",
        "    elif optimizer==\"nadam\":\n",
        "        gradient = gradient_descent_nadam_n\n",
        "    else:\n",
        "        gradient = gradient_descent_stochastic\n",
        "\n",
        "    Weights, Biases = initialize(num_of_hidden_layers+2,size_of_layers,Y.shape[1], X.shape[1])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch number\", epoch+1, \" started\")\n",
        "        Weights, Biases = gradient(X, Y, learning_rate, num_of_hidden_layers+2, batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, beta1, epsilon)\n",
        "        print(\"Epoch \",epoch+1, \" finished.\")\n",
        "        # Y_predict = []\n",
        "        # for i in validatex:\n",
        "        #     Y_predict.append(validate(i, Weights, Biases, activation_function))\n",
        "        Y_predict = validate_n(validatex,Weights,Biases,activation_function)\n",
        "        print(\"validation accuracy after epoch :\",get_accuracy(validatey,Y_predict),\"%\")\n",
        "        print(\"Validation Loss (cross entropy)\",cross_entropy_loss( validatey,Y_predict))\n",
        "        print(\"Validation Loss (mean square)\",mean_squared_error( validatey,Y_predict))\n",
        "\n",
        "    return Weights, Biases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------\n",
        "\n",
        "# testing\n",
        "\n",
        "\n",
        "def validate(X, Weights, Biases, activation_function):\n",
        "    # some calculations\n",
        "    H, A, Y_pred = forward_propagation(X, Weights=Weights, Biases=Biases, number_of_layers=len(Weights)+1, activation_function=activation_function)\n",
        "    return Y_pred\n",
        "\n",
        "\n",
        "def validate_n(X, Weights, Biases, activation_function):\n",
        "    # some calculations\n",
        "    H, A, Y_pred = forward_propagation_n(X.transpose(), Weights=Weights, Biases=Biases, number_of_layers=len(Weights)+1, activation_function=activation_function, batch_size=X.shape[0])\n",
        "    return Y_pred.transpose()\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# loss\n",
        "\n",
        "def cross_entropy_loss( y_actual, y_pred):\n",
        "    loss = 0\n",
        "    for i in range(len(y_actual)):\n",
        "        loss = loss + (-np.log(y_pred[i][np.argmax(y_actual[i])]))\n",
        "    return loss/len(y_actual)\n",
        "\n",
        "def mean_squared_error( y_actual, y_pred):\n",
        "    loss = 0\n",
        "    for i in range(len(y_actual)):\n",
        "        for j in range(len(y_actual[i])):\n",
        "            loss = loss + (y_actual[i][j]-y_pred[i][j])**2\n",
        "    loss = loss / (len(y_actual)*len(y_actual[0]))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c_TnAlNi55AY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a176b9f-94f2-4bd2-d8d7-fee2237a91a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Taking input data and normalizing data\n",
        "\n",
        "\n",
        "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "validateX = trainX[54000:]\n",
        "validateY = trainY[54000:]\n",
        "trainX = trainX[:54000]\n",
        "trainY = trainY[:54000]\n",
        "\n",
        "trainx = trainX.reshape(trainX.shape[0],-1)/255\n",
        "validatex = validateX.reshape(validateX.shape[0],-1)/255\n",
        "testx = testX.reshape(testX.shape[0],-1)/255\n",
        "\n",
        "# output dataset conversion One hot encoding\n",
        "import numpy as np\n",
        "trainy = [np.zeros(10) for i in range(trainX.shape[0])]\n",
        "validatey = [np.zeros(10) for i in range(validateX.shape[0])]\n",
        "testy = [np.zeros(10) for i in range(testX.shape[0])]\n",
        "\n",
        "for i in range(trainX.shape[0]):\n",
        "    trainy[i][trainY[i]] = 1\n",
        "\n",
        "for i in range(validateX.shape[0]):\n",
        "    validatey[i][validateY[i]] = 1\n",
        "\n",
        "for i in range(testX.shape[0]):\n",
        "    testy[i][testY[i]] = 1\n",
        "\n",
        "trainy = np.array(trainy)\n",
        "testy = np.array(testy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "g2PMQDnD55AZ",
        "outputId": "1307b974-2b12-4ce7-f156-7af3a5abd644"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display few images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cnt = 0\n",
        "label_display_name = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "label = []\n",
        "\n",
        "for i in range(1, trainX.shape[0]):\n",
        "\n",
        "    if trainY[i] not in label:\n",
        "        cnt = cnt + 1\n",
        "        label.append(trainY[i])\n",
        "        plt.subplot(2, 5, cnt)\n",
        "        # Insert ith image with the color map 'grap'\n",
        "        plt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
        "        plt.title(label_display_name[trainY[i]])\n",
        "\n",
        "    if cnt == 10:\n",
        "        break\n",
        "\n",
        "# Display the entire plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOE3NP6I55AZ",
        "outputId": "5fdd6c75-cdbb-41c0-831d-6cccf9cc8605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training datapoints: 54000\n",
            "number of epochs: 5\n",
            "number of hidden layers: 4\n",
            "size of hidden layers: 32\n",
            "learning rate: 0.001\n",
            "optimizer: nadam\n",
            "batch_size: 50\n",
            "l2 regularization constant: 0.0005\n",
            "weights and biases initialization type: xavier\n",
            "activation function: sigmoid\n",
            "beta1: 0.9\n",
            "beta2: 0.999\n",
            "epsilon: 0.0001\n",
            "Epoch number 1  started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1080/1080 [00:12<00:00, 83.55it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1  finished.\n",
            "total data points in validation  6000\n",
            "validation accuracy after epoch : 37.95 %\n",
            "Validation Loss (cross entropy) 1.4311573023474191\n",
            "Validation Loss (mean square) 0.07011144335977502\n",
            "Epoch number 2  started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1080/1080 [00:11<00:00, 96.62it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2  finished.\n",
            "total data points in validation  6000\n",
            "validation accuracy after epoch : 71.95 %\n",
            "Validation Loss (cross entropy) 0.7135727529070471\n",
            "Validation Loss (mean square) 0.03677711103183466\n",
            "Epoch number 3  started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1080/1080 [00:10<00:00, 100.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3  finished.\n",
            "total data points in validation  6000\n",
            "validation accuracy after epoch : 79.23333333333333 %\n",
            "Validation Loss (cross entropy) 0.5593473477048623\n",
            "Validation Loss (mean square) 0.02821427324482932\n",
            "Epoch number 4  started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1080/1080 [00:10<00:00, 107.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4  finished.\n",
            "total data points in validation  6000\n",
            "validation accuracy after epoch : 81.2 %\n",
            "Validation Loss (cross entropy) 0.5107987514160863\n",
            "Validation Loss (mean square) 0.02599976818068491\n",
            "Epoch number 5  started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1080/1080 [00:11<00:00, 96.40it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5  finished.\n",
            "total data points in validation  6000\n",
            "validation accuracy after epoch : 81.89999999999999 %\n",
            "Validation Loss (cross entropy) 0.4838067751006482\n",
            "Validation Loss (mean square) 0.024921859052438756\n"
          ]
        }
      ],
      "source": [
        "## Training the model\n",
        "X = trainx\n",
        "Y = trainy\n",
        "epochs = 5\n",
        "num_of_hidden_layers = 4\n",
        "size_of_layers = 32\n",
        "learning_rate = 0.001\n",
        "optimizer = \"nadam\"\n",
        "batch_size = 50\n",
        "l2_regularization_constant = 0.0005\n",
        "weight_init_type = \"xavier\"\n",
        "activation_function = \"sigmoid\"\n",
        "beta = 0.9\n",
        "beta1 = 0.999\n",
        "epsilon = 0.0001\n",
        "\n",
        "\n",
        "\n",
        "Weights, Biases = train_model(\n",
        "                        X=X,\n",
        "                        Y=Y,\n",
        "                        epochs=epochs,\n",
        "                        num_of_hidden_layers=num_of_hidden_layers,\n",
        "                        size_of_layers=size_of_layers,\n",
        "                        learning_rate=learning_rate,\n",
        "                        optimizer=optimizer,\n",
        "                        batch_size=batch_size,\n",
        "                        l2_regularization_constant=l2_regularization_constant,\n",
        "                        weight_init_type=weight_init_type,\n",
        "                        activation_function=activation_function,\n",
        "                        beta=beta,\n",
        "                        beta1=beta1,\n",
        "                        epsilon=epsilon\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNeuralNetwork:\n",
        "    def __inti__(self, ):\n",
        "        pass"
      ],
      "metadata": {
        "id": "dWG_IUy0AZuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hat4ZgDSAlwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLKeoaH9Al08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY42SROQ55AZ",
        "outputId": "d691bec0-5389-45d1-f2a4-18f4c5bd05db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy 71.3\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "Y_predict = []\n",
        "for i in testx:\n",
        "    Y_predict.append(validate(i, Weights, Biases, activation_function))\n",
        "\n",
        "print(\"test accuracy\",get_accuracy(testy,Y_predict))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in Weights:\n",
        "    for j in i:\n",
        "        print(j)\n",
        "    break"
      ],
      "metadata": {
        "id": "Y4KhgULb2WBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "nodes_per_hidden_layer = 32\n",
        "input_layer_size = 784\n",
        "number_of_layers = 5\n",
        "nodes_in_output_layer = 10\n",
        "batch_size = 50\n",
        "\n",
        "WS = np.random.randn(nodes_per_hidden_layer, input_layer_size)\n",
        "W = np.random.randn(number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer)\n",
        "B = np.random.randn(number_of_layers-2, 1, nodes_per_hidden_layer)\n",
        "WL = np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)\n",
        "BL = np.random.randn(1, 1, nodes_in_output_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# BL = np.repeat(BL,batch_size,axis=1)\n",
        "\n",
        "Weights = [WS] + [i for i in W] + [WL]\n",
        "Biases = [i for i in B] + [i for i in BL]\n",
        "\n",
        "for i in range(len(Biases)):\n",
        "    Biases[i] = np.repeat(Biases[i].transpose(),batch_size, axis=1)\n",
        "\n",
        "for i in range(len(Biases)):\n",
        "    print(Biases[i].shape)\n",
        "\n",
        "print(\"Weights\")\n",
        "\n",
        "for i in range(len(Weights)):\n",
        "    print(Weights[i].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kCBFyuCK1E0",
        "outputId": "0be3dd23-0b7f-4ac6-d7e4-e1fb5226f75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 50)\n",
            "(32, 50)\n",
            "(32, 50)\n",
            "(10, 50)\n",
            "Weights\n",
            "(32, 784)\n",
            "(32, 32)\n",
            "(32, 32)\n",
            "(10, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD4z-nbs2Jgt",
        "outputId": "8674d8a1-4c80-410e-b951-60cd3e1d2973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
              " array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        ...,\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan],\n",
              "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
              " array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan]])]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Weights[1][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XbcKC7i2Nr3",
        "outputId": "12b59e5f-90e6-4720-f932-5bfa944bf25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "       nan, nan, nan, nan, nan, nan])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}