{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All functions required\n",
    "\n",
    "# Activation functions\n",
    "# 1 Relu\n",
    "def relu(a):\n",
    "    if a < 0 :\n",
    "        return 0\n",
    "    return a\n",
    "\n",
    "def relu_vector(a):\n",
    "    temp = []\n",
    "    for i in range(len(a)):\n",
    "        temp.append(relu(a[i]))\n",
    "    return temp\n",
    "\n",
    "# 2 Sigmoid\n",
    "def sigmoid(a):\n",
    "    ans = 1/(1+np.exp(-a))\n",
    "    return ans\n",
    "\n",
    "def sigmoid_vector_old(a):\n",
    "    temp=[]\n",
    "    for i in range(len(a)):\n",
    "        temp.append(sigmoid(a[i]))\n",
    "    return temp\n",
    "\n",
    "def sigmoid_vector(a):\n",
    "    a = np.clip(a, -200,200)\n",
    "    ans = 1/(1+np.exp(-a))\n",
    "    return ans\n",
    "\n",
    "# 3 Tanh\n",
    "def tanh(a):\n",
    "    a = np.clip(a, -200, 200)\n",
    "    ans = (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "    return ans\n",
    "\n",
    "def tanh_vector_old(a):\n",
    "    temp = []\n",
    "    for i in range(len(a)):\n",
    "        temp.append(tanh(a[i]))\n",
    "    return temp\n",
    "\n",
    "def tanh_vector(a):\n",
    "    a = np.clip(a, -200, 200)\n",
    "    ans = (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))\n",
    "    return ans\n",
    "\n",
    "# Output Activation Function\n",
    "\n",
    "# Softmax\n",
    "def softmax(a):\n",
    "    a = np.clip(a, -200, 200)\n",
    "    return np.exp(a)/np.sum(np.exp(a))\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# utility functions\n",
    "\n",
    "def hadamard_product(A,B):\n",
    "    result = []\n",
    "    for i in range(len(A)):\n",
    "        result.append(A[i]*B[i])\n",
    "\n",
    "    return result\n",
    "\n",
    "def random_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
    "    if number_of_layers<=2:\n",
    "        return [np.random.randn(nodes_in_output_layer,input_layer_size)], [np.random.randn(nodes_in_output_layer)]\n",
    "\n",
    "    if number_of_layers==3:\n",
    "        Weights = [np.random.randn(nodes_per_hidden_layer, input_layer_size), np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)]\n",
    "        Biases = [np.random.randn(nodes_per_hidden_layer), np.random.randn(nodes_in_output_layer)]\n",
    "        return Weights, Biases\n",
    "\n",
    "    WS = np.random.randn(nodes_per_hidden_layer, input_layer_size)\n",
    "    W = np.random.randn(number_of_layers-3, nodes_per_hidden_layer ,nodes_per_hidden_layer)\n",
    "    B = np.random.randn(number_of_layers-2, nodes_per_hidden_layer)\n",
    "    WL = np.random.randn(nodes_in_output_layer, nodes_per_hidden_layer)\n",
    "    BL = np.random.randn(nodes_in_output_layer)\n",
    "\n",
    "    Weights = [WS] + [i for i in W] + [WL]\n",
    "    Biases = [i for i in B] + [BL]\n",
    "    return Weights, Biases\n",
    "\n",
    "def xavier_initialize(number_of_layers, nodes_per_hidden_layer, nodes_in_output_layer, input_layer_size=784):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_accuracy(Y_actual, Y_predicted):\n",
    "    total = len(Y_actual)\n",
    "    cnt = 0\n",
    "    for i in range(total):\n",
    "        if np.argmax(Y_actual[i]) == np.argmax(Y_predicted[i]):\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return (cnt/total)*100\n",
    "\n",
    "\n",
    "def get_average_delta_WandB(delta_W_acc, delta_B_acc):\n",
    "    for i in range(1,len(delta_W_acc)):\n",
    "        for j in range(len(delta_W_acc[0])):\n",
    "            delta_W_acc[0][j] = np.add(delta_W_acc[0][j] , delta_W_acc[i][j])\n",
    "            # if i==len(delta_W_acc)-1:\n",
    "            #     delta_W_acc[0][j] = delta_W_acc[0][j] / len(delta_W_acc)\n",
    "\n",
    "        for j in range(len(delta_B_acc[0])):\n",
    "            delta_B_acc[0][j] = np.add(delta_B_acc[0][j] , delta_B_acc[i][j])\n",
    "            # if i==len(delta_B_acc)-1:\n",
    "            #     delta_B_acc[0][j] = delta_B_acc[0][j] / len(delta_B_acc)\n",
    "\n",
    "    return delta_W_acc[0], delta_B_acc[0]\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "# Derivative functions\n",
    "\n",
    "def derivative_sigmoid(a):\n",
    "    return sigmoid(a) * (1-sigmoid(a))\n",
    "\n",
    "def derivative_tanh(a):\n",
    "    return 1 - (tanh(a)**2)\n",
    "\n",
    "def derivative_relu(a):\n",
    "    if a<=0:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Pre Avtivation function\n",
    "\n",
    "\n",
    "def pre_activation(W, h, b):\n",
    "    return np.add(np.matmul(W,h) , b)\n",
    "\n",
    "\n",
    "#------------------------------------------------------\n",
    "\n",
    "# forward propagation \n",
    "\n",
    "\n",
    "def forward_propagation(X, Weights, Biases, number_of_layers, activation_function):\n",
    "    \n",
    "    if activation_function==\"relu\":\n",
    "        activation = relu_vector\n",
    "    elif activation_function==\"tanh\":\n",
    "        activation = tanh_vector\n",
    "    else:\n",
    "        activation = sigmoid_vector\n",
    "    \n",
    "    A = []\n",
    "    H = [X]\n",
    "    for i in range(number_of_layers-2):\n",
    "        A.append(pre_activation(Weights[i],H[i],Biases[i]))\n",
    "        H.append(activation(A[i]))\n",
    "\n",
    "    A.append(pre_activation(Weights[-1], H[-1], Biases[-1]))\n",
    "\n",
    "    y_pred = softmax(A[-1])\n",
    "\n",
    "    return H, A, y_pred\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Backward Propagation\n",
    "\n",
    "def backward_propagation(H, A, W, y_actual, y_pred, number_of_layers, activation_function):\n",
    "    \n",
    "    if activation_function==\"relu\":\n",
    "        derivative = derivative_relu\n",
    "    elif activation_function==\"tanh\":\n",
    "        derivative = derivative_tanh\n",
    "    else:\n",
    "        derivative = derivative_sigmoid\n",
    "    # delta_A = [0 for i in range(number_of_layers-1)]\n",
    "    delta_W = [0 for i in range(number_of_layers-1)]\n",
    "    delta_B = [0 for i in range(number_of_layers-1)]\n",
    "    # delta_H = [0 for i in range(number_of_layers-2)]\n",
    "\n",
    "    # gradient with respect to output\n",
    "    # delta_A[-1] = -(y_actual-y_pred)\n",
    "    delta_A = -(y_actual-y_pred)\n",
    "    delta_H = None\n",
    "\n",
    "\n",
    "    for k in reversed(range(number_of_layers-1)):\n",
    "\n",
    "        # gradient with respect to parameters\n",
    "        # delta_W[k] = np.outer(delta_A[k],H[k-1])\n",
    "        delta_W[k] = np.outer(delta_A, H[k])\n",
    "        # delta_B[k] = delta_A[k]\n",
    "        delta_B[k] = delta_A\n",
    "\n",
    "        if k==0:\n",
    "            break\n",
    "        # gradient with respect to layer below\n",
    "        # delta_H[k-1] = np.matmul(W[k].transpose() , delta_A[k])\n",
    "        delta_H = np.matmul(W[k].transpose() , delta_A)\n",
    "\n",
    "        #gradient with respect to layer below (i.e. pre-activation)\n",
    "        # delta_A[k-1] = hadamard_product(delta_H[k-1],[derivative(i) for i in A[k-1]])\n",
    "        delta_A = hadamard_product(delta_H,[derivative(i) for i in A[k-1]])\n",
    "        \n",
    "\n",
    "    return delta_W, delta_B\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# gradient descent algorithms\n",
    "\n",
    "\n",
    "def update_weights_and_biases(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant):\n",
    "    for i in range(len(Weights)):\n",
    "        # Weights[i] = Weights[i] - learning_rate * delta_Weights[i]\n",
    "        # Biases[i] = Biases[i] - learning_rate * delta_Biases[i]\n",
    "        for j in range(len(Weights[i])):\n",
    "            Weights[i][j] = Weights[i][j] - learning_rate * delta_Weights[i][j] - (learning_rate * l2_regularization_constant * Weights[i][j])\n",
    "\n",
    "        for j in range(len(Biases[i])):\n",
    "            Biases[i][j] = Biases[i][j] - learning_rate * delta_Biases[i][j] - (learning_rate * l2_regularization_constant * Biases[i][j])\n",
    "\n",
    "    return Weights, Biases\n",
    "\n",
    "\n",
    "def gradient_descent_stochastic(X, Y, learning_rate, number_of_layers, batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, epsilon=0):\n",
    "\n",
    "    # Weights, Biases = random_initialize(number_of_layers,nodes_per_hidden_layer,nodes_in_output_layer)\n",
    "    itr = 0\n",
    "\n",
    "    while itr<X.shape[0]:\n",
    "        # H, A, y_pred = forward_propagation(X[itr*batch_size:(itr+1)*batch_size], Weights, Biases, number_of_layers)\n",
    "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
    "        # return None, None\n",
    "        # delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr*batch_size:(itr+1)*batch_size], y_pred, number_of_layers)\n",
    "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)        \n",
    "        Weights , Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_Weights, delta_Biases, l2_regularization_constant)\n",
    "        itr = itr + 1\n",
    "    return Weights, Biases\n",
    "\n",
    "def gradient_descent_mini_batch(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, epsilon=0):\n",
    "    itr = 0\n",
    "    delta_W_acc = []\n",
    "    delta_B_acc = []\n",
    "\n",
    "    while itr<X.shape[0]:\n",
    "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
    "\n",
    "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
    "\n",
    "        delta_W_acc.append(delta_Weights)\n",
    "        delta_B_acc.append(delta_Biases)\n",
    "\n",
    "        itr = itr + 1\n",
    "        if itr%batch_size==0:\n",
    "            delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
    "\n",
    "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
    "\n",
    "            delta_W_acc = []\n",
    "            delta_B_acc = []\n",
    "            delta_W_avg = 0\n",
    "            delta_B_avg = 0\n",
    "\n",
    "\n",
    "    if delta_B_acc and delta_W_acc:\n",
    "        delta_W_avg, delta_B_avg = get_average_delta_WandB(delta_W_acc, delta_B_acc)\n",
    "\n",
    "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, delta_W_avg, delta_B_avg, l2_regularization_constant)\n",
    "\n",
    "    return Weights, Biases\n",
    "\n",
    "def accumulate_history(prev, current, prev_factor=1, current_factor=1):\n",
    "    for i in range(len(prev)):\n",
    "        prev[i] = np.add(prev[i]*prev_factor, current[i]*current_factor)\n",
    "\n",
    "    return prev\n",
    "\n",
    "def gradient_descent_momentum_based(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon=0):\n",
    "    \n",
    "    itr = 0\n",
    "    u_t_weights = np.zeros_like(Weights)\n",
    "    u_t_biases = np.zeros_like(Biases)\n",
    "    # u_t_list = [u_t]\n",
    "    while itr<X.shape[0]:\n",
    "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
    "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
    "        # u_t = beta * u_t + delta_Weights\n",
    "        u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
    "        u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
    "        # u_t_list.append()\n",
    "        itr = itr + 1\n",
    "\n",
    "        if itr%batch_size==0:\n",
    "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
    "\n",
    "    if itr%batch_size!=0:\n",
    "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
    "\n",
    "    return Weights, Biases\n",
    "\n",
    "\n",
    "def square_each_term(a):\n",
    "    temp = []\n",
    "    for i in range(len(a)):\n",
    "        temp.append(np.multiply(a[i],a[i]))\n",
    "    return temp\n",
    "\n",
    "def modify_deltas_RMSProp(v_t, w_t, epsilon):\n",
    "    temp = []\n",
    "    for i in range(len(v_t)):\n",
    "        temp.append(np.divide(w_t[i], (np.sqrt(v_t[i]) + epsilon)))\n",
    "    return temp\n",
    "\n",
    "def gradient_descent_RMSProp(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon):\n",
    "    \n",
    "    itr = 0\n",
    "    v_t_weights = np.zeros_like(Weights)\n",
    "    v_t_biases = np.zeros_like(Biases)\n",
    "\n",
    "    while itr<X.shape[0]:\n",
    "        H, A, y_pred = forward_propagation(X[itr], Weights, Biases, number_of_layers, activation_function)\n",
    "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
    "\n",
    "        v_t_weights = accumulate_history(v_t_weights,square_each_term(delta_Weights),prev_factor=beta, current_factor=1-beta)\n",
    "        v_t_biases = accumulate_history(v_t_biases,square_each_term(delta_Biases), prev_factor=beta, current_factor=1-beta)\n",
    "\n",
    "        itr = itr + 1\n",
    "\n",
    "        if itr%batch_size==0:\n",
    "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
    "    \n",
    "    if itr%batch_size!=0:\n",
    "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, modify_deltas_RMSProp(v_t_weights, delta_Weights, epsilon), modify_deltas_RMSProp(v_t_biases, delta_Biases, epsilon),l2_regularization_constant)\n",
    "    \n",
    "    \n",
    "    return Weights, Biases\n",
    "\n",
    "\n",
    "def modify_W_B_NAGD(u_t, w_t, beta):\n",
    "    temp = []\n",
    "    for i in range(len(u_t)):\n",
    "        temp.append(np.subtract(w_t[i],beta*u_t[i]))\n",
    "    return temp\n",
    "\n",
    "\n",
    "def gradient_descent_nesterov_accelarated(X, Y, learning_rate, number_of_layers,  batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta, epsilon=0):\n",
    "    itr = 0\n",
    "    u_t_weights = np.zeros_like(Weights)\n",
    "    u_t_biases = np.zeros_like(Biases)\n",
    "    # u_t_list = [u_t]\n",
    "    while itr<X.shape[0]:\n",
    "        H, A, y_pred = forward_propagation(X[itr], modify_W_B_NAGD(u_t_weights, Weights, beta), modify_W_B_NAGD(u_t_biases, Biases, beta), number_of_layers, activation_function)\n",
    "        delta_Weights, delta_Biases = backward_propagation(H, A, Weights, Y[itr], y_pred, number_of_layers, activation_function)\n",
    "        # u_t = beta * u_t + delta_Weights\n",
    "        u_t_weights = accumulate_history(u_t_weights,delta_Weights,prev_factor=beta)\n",
    "        u_t_biases = accumulate_history(u_t_biases,delta_Biases, prev_factor=beta)\n",
    "        # u_t_list.append()\n",
    "        itr = itr + 1\n",
    "\n",
    "        if itr%batch_size==0:\n",
    "            Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
    "\n",
    "    if itr%batch_size!=0:\n",
    "        Weights, Biases = update_weights_and_biases(learning_rate, Weights, Biases, u_t_weights, u_t_biases, l2_regularization_constant)\n",
    "\n",
    "\n",
    "    return Weights, Biases\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "#training\n",
    "\n",
    "\n",
    "\n",
    "def train_model(X, Y, epochs=1, num_of_hidden_layers=1, size_of_layers=4, learning_rate=0.1, optimizer=\"sgd\", batch_size=4, l2_regularization_constant=0.001, weight_init_type=\"random\", activation_function=\"sigmoid\", beta=0, epsilon=1e-10):\n",
    "\n",
    "    if weight_init_type==\"random\":\n",
    "        initialize = random_initialize\n",
    "    else:\n",
    "        initialize = xavier_initialize\n",
    "\n",
    "    if optimizer==\"mini_batch\":\n",
    "        gradient = gradient_descent_mini_batch\n",
    "    elif optimizer==\"mbgd\":\n",
    "        gradient = gradient_descent_momentum_based\n",
    "    elif optimizer==\"rmsprop\":\n",
    "        gradient = gradient_descent_RMSProp\n",
    "    elif optimizer==\"nagd\":\n",
    "        gradient = gradient_descent_nesterov_accelarated\n",
    "    else:\n",
    "        gradient = gradient_descent_stochastic\n",
    "\n",
    "    Weights, Biases = initialize(num_of_hidden_layers+2,size_of_layers,Y.shape[1], X.shape[1])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch number\", epoch+1, \" started\")\n",
    "        Weights, Biases = gradient(X, Y, learning_rate, num_of_hidden_layers+2, batch_size, Weights, Biases, activation_function, l2_regularization_constant, beta=0, epsilon=0)\n",
    "        print(\"Epoch \",epoch+1, \" finished.\")\n",
    "    return Weights, Biases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# testing \n",
    "\n",
    "\n",
    "def validate(X, Weights, Biases, activation_function):\n",
    "    # some calculations\n",
    "    H, A, Y_pred = forward_propagation(X, Weights=Weights, Biases=Biases, number_of_layers=len(Weights)+1, activation_function=activation_function)\n",
    "    return Y_pred\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking input data and normalizing data\n",
    "\n",
    "\n",
    "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "validateX = trainX[54000:]\n",
    "validateY = trainY[54000:]\n",
    "trainX = trainX[:54000]\n",
    "trainY = trainY[:54000]\n",
    "\n",
    "trainx = trainX.reshape(trainX.shape[0],-1)/255\n",
    "validatex = validateX.reshape(validateX.shape[0],-1)/255\n",
    "testx = testX.reshape(testX.shape[0],-1)/255\n",
    "\n",
    "# output dataset conversion One hot encoding\n",
    "import numpy as np\n",
    "trainy = [np.zeros(10) for i in range(trainX.shape[0])]\n",
    "validatey = [np.zeros(10) for i in range(validateX.shape[0])]\n",
    "testy = [np.zeros(10) for i in range(testX.shape[0])]\n",
    "\n",
    "for i in range(trainX.shape[0]):\n",
    "    trainy[i][trainY[i]] = 1\n",
    "    \n",
    "for i in range(validateX.shape[0]):\n",
    "    validatey[i][validateY[i]] = 1\n",
    "    \n",
    "for i in range(testX.shape[0]):\n",
    "    testy[i][testY[i]] = 1\n",
    "    \n",
    "trainy = np.array(trainy)\n",
    "testy = np.array(testy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display few images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cnt = 0\n",
    "label_display_name = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "label = []\n",
    "\n",
    "for i in range(1, trainX.shape[0]):\n",
    "\n",
    "    if trainY[i] not in label:\n",
    "        cnt = cnt + 1\n",
    "        label.append(trainY[i])\n",
    "        plt.subplot(2, 5, cnt)\n",
    "        # Insert ith image with the color map 'grap'\n",
    "        plt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
    "        plt.title(label_display_name[trainY[i]])\n",
    "\n",
    "    if cnt == 10:\n",
    "        break\n",
    "\n",
    "# Display the entire plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the model\n",
    "X = trainx,\n",
    "Y = trainy,\n",
    "epochs = 1,\n",
    "num_of_hidden_layers = 1,\n",
    "size_of_layers = 4, \n",
    "learning_rate = 0.1,\n",
    "optimizer = \"sgd\",\n",
    "batch_size = 4,\n",
    "l2_regularization_constant = 0.001,\n",
    "weight_init_type = \"random\",\n",
    "activation_function = \"sigmoid\",\n",
    "beta = 0,\n",
    "epsilon = 1e-10\n",
    "\n",
    "\n",
    "\n",
    "Weights, Biases = train_model(\n",
    "                        X=X,\n",
    "                        Y=Y,\n",
    "                        epochs=epochs,\n",
    "                        num_of_hidden_layers=num_of_hidden_layers,\n",
    "                        size_of_layers=size_of_layers,\n",
    "                        learning_rate=learning_rate,\n",
    "                        optimizer=optimizer,\n",
    "                        batch_size=batch_size,\n",
    "                        l2_regularization_constant=l2_regularization_constant,\n",
    "                        weight_init_type=weight_init_type,\n",
    "                        activation_function=activation_function,\n",
    "                        beta=beta,\n",
    "                        epsilon=epsilon\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation \n",
    "Y_predict = []\n",
    "for i in validateX:\n",
    "    Y_predict.append(validate(i, Weights, Biases))\n",
    "\n",
    "print(\"validation accuracy\",get_accuracy(validatey,Y_predict))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
